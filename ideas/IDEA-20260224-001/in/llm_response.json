{
"meta": {
"language": "ru",
"assumptions": [
"Цель — не «доказать, что LLM хороши», а построить воспроизводимый протокол оценки надежности (quality + risk bounds) для LLM/ChatGPT в цепочках evidence-review (поиск → скрининг → извлечение → синтез).",
"Под «надежностью» понимаются минимум 3 класса ошибок: (1) ошибки отбора (ложные исключения/включения), (2) фактические ошибки/«галлюцинации» в резюме и выводах, (3) некорректные/неподдерживающие источники (цитата не подтверждает утверждение).",
"Рассматривается типичная современная практика систематических обзоров с опорой на PRISMA 2020 как стандарт отчетности процесса (а не как «метод лечения»). ",
"Оцениваемые сценарии включают: LLM-only; LLM + структурированные промпты; LLM + RAG/доказательная подкладка; LLM + автоматическая верификация цитат; human-in-the-loop (эскалация пограничных случаев)."
],
"missing_critical_info": [
"Какой этап/этапы обзора приоритетны: только скрининг (title/abstract) или также data extraction, суммаризация, формулировка выводов.",
"Допустимый профиль риска: что хуже — пропустить релевантные работы (FN) или включить лишние (FP); какие пороги чувствительности/специфичности приемлемы.",
"Целевая область: биомедицина/экология/соцнауки/инженерия (сильно влияет на распределение терминов, наличие «золотых стандартов» и стоимость ошибки).",
"Требования к воспроизводимости: фиксирование версий моделей, температур, промптов, источников, а также допустимая вариативность результатов между запусками."
],
"what_would_change_conclusions_most": [
"Результаты много-доменных бенчмарков на реальных наборах для скрининга систематических обзоров (precision/recall/F1, стоимость, время, стабильность). ",
"Доказательства того, что RAG и/или пост-верификация реально снижают долю фактических ошибок и «неподдержанных цитат» (а не просто меняют стиль ответа). ",
"Надежность детекции галлюцинаций: если метрики/детекторы нестабильны и плохо обобщаются, то любые «автоматические гарантии» надежности рушатся. "
]
},
"structured_idea": {
"problem": "LLM/ChatGPT активно внедряются в этапы evidence-review (скрининг, извлечение, суммаризация), но их ошибки (галлюцинации, неверные/неподдерживающие ссылки, нестабильность между запусками) могут систематически искажать выводы обзора. Нужен строгий, воспроизводимый протокол оценки надежности и набор «предохранителей» (RAG, верификация, human-in-the-loop), который дает измеримые и проверяемые границы ошибок по ключевым этапам.",
"why_it_matters": "Систематические обзоры используются для принятия решений (наука, политика, клиника). Ошибка LLM может быть незаметной, но критичной: пропуск ключевых исследований, ложное обобщение, «фальшивая уверенность» с правдоподобными ссылками. Стандарты отчетности (PRISMA 2020) требуют прозрачности процесса — LLM-пайплайн должен быть совместим с этими требованиями и проверяем. ",
"state_of_practice_today": "PRISMA 2020 задает современный стандарт отчетности систематических обзоров (что делали, как искали, как отбирали и т.д.). Одновременно появляются работы и обзоры по применению LLM для скрининга: показывают, что качество сильно зависит от модели и типа промпта, возможны быстрые/дешевые режимы первой проходки и эскалация пограничных случаев. По надежности фактов и галлюцинациям есть развитая таксономия и обсуждение, но измерение и детекция галлюцинаций остаются методически сложными: разные метрики могут быть слабо согласованы и плохо обобщаться между датасетами. RAG и знание-ориентированные подходы часто рассматриваются как путь снижения галлюцинаций за счет привязки генерации к извлеченным источникам, но эффект зависит от качества ретривера и процедуры проверки. ",
"main_hypothesis": "H1: Комбинация (A) структурированных промптов, (B) RAG/извлечения первоисточников, (C) автоматической верификации поддержки утверждений ссылками и (D) human-in-the-loop эскалации пограничных случаев позволяет построить LLM-assisted evidence-review пайплайн, который достигает заданных порогов надежности (высокая чувствительность на скрининге + низкая доля неподдержанных утверждений в суммаризации) при меньших затратах времени, чем полностью ручной процесс.",
"alternative_hypotheses": [
"H2: Основная прибавка качества определяется не RAG/верификацией, а правильной постановкой задачи и протоколом (порогами, двухпроходной схемой, четкими критериями включения/исключения); LLM дает сравнимый результат с традиционными ML-инструментами скрининга/правилами, а RAG добавляет сложность без стабильного выигрыша.",
"H3: Надежность LLM принципиально ограничена: даже при RAG и верификации сохраняется неприемлемая доля фактических/цитатных ошибок и/или высокая нестабильность между запусками; LLM допустимы лишь как ускоритель (черновики/подсказки), но не как компонент принятия решений без существенного ручного контроля."
],
"key_predictions": [
"Если H1 верна, то режим LLM+RAG+верификация покажет (i) меньшую долю неподдержанных утверждений и неверных цитат по сравнению с LLM-only и (ii) сопоставимую/лучшую чувствительность на скрининге при приемлемой специфичности; при этом выигрыш по времени/стоимости будет значимым. ",
"Если H2 верна, то лучшие результаты будут достигаться главным образом сменой протокола (двухпроходность, строгие критерии, эскалация), а добавление RAG/сложной верификации даст слабый или непостоянный прирост, особенно на разных доменах.",
"Если H3 верна, то даже при RAG/верификации останутся (a) существенная доля неподдержанных утверждений/цитат и/или (b) высокая межзапусковая вариативность решений и выводов; детекторы галлюцинаций будут плохо коррелировать и не обеспечат надежных гарантий. "
],
"decisive_tests": [
{
"test": "Много-доменный проспективный бенчмарк на скрининге (title/abstract) с протоколами: Human-only vs LLM-only vs LLM+structured prompts vs LLM+эскалация пограничных случаев vs LLM+RAG (если доступен full-text) + верификация. Метрики: recall/sensitivity при фиксированном precision (или наоборот), время, стоимость, межзапусковая стабильность.",
"data_needed": "Наборы с «золотым стандартом» включения/исключения (реальные проекты систематических обзоров или публичные датасеты); четко заданные критерии включения/исключения; логи решений по каждой записи; фиксированные версии моделей/параметров. Для сопоставимости — одинаковый корпус и одинаковые критерии для всех условий.",
"analysis": "Оценить precision/recall/F1 и cost-time tradeoff; построить кривые PR или Recall@Precision; измерить межзапусковую согласованность (Cohen’s kappa/percent agreement) при повторных запусках; провести стратификацию по доменам и по сложности абстрактов. Сравнить лучшие практики промптинга/двухпроходности, отмеченные в работах по LLM-скринингу. ",
"expected_patterns_by_hypothesis": {
"H1": "LLM+structured+эскалация и/или LLM+RAG покажут лучшую или равную чувствительность при заданной специфичности по сравнению с LLM-only, а также приемлемую стабильность; стоимость/время будут существенно ниже human-only.",
"H2": "Сильный прирост даст именно протокол (двухпроходность, эскалация), а RAG/верификация дадут небольшой/нестабильный эффект; LLM будет сопоставим с традиционными ML-скринингами по качеству, но не радикально лучше.",
"H3": "LLM-режимы будут либо нестабильны (низкая воспроизводимость), либо не достигнут требуемых порогов recall/precision без массивного ручного контроля; преимущества по времени будут нивелированы нуждой в перепроверке."
},
"midterm_exam": "Показать на 2 доменах и 2 независимых наборах, что выбранный протокол достигает заранее заданного порога recall (например, ≥0.95 на первом проходе) при контролируемом росте FP и измеримой стабильности между 3 запусками.",
"final_exam": "Демонстрация воспроизводимого протокола (полные логи, фиксация параметров) и итогового профиля риска/стоимости: какой режим допустим для реального evidence-review при заданных порогах качества и где требуются обязательные ручные проверки."
},
{
"test": "Тест надежности фактов и цитирования в суммаризации/извлечении: сравнить LLM-only vs LLM+RAG vs LLM+RAG+пост-верификация (проверка, что каждое ключевое утверждение поддержано извлеченным фрагментом источника). Метрики: доля неподдержанных утверждений, доля неверных цитат, доля «переформулировок без опоры», стабильность между запусками.",
"data_needed": "Подвыборка статей с доступным full-text (PDF/HTML); заранее определенные «контрольные вопросы» и шаблон извлечения (PICO/или аналог); эталонная разметка поддержки утверждений (human adjudication на подмножестве) + автоматические проверки (retrieval + entailment/LLM-as-judge с валидацией).",
"analysis": "Разбить ответы на атомарные утверждения; для каждого утверждения найти поддерживающий спан в источнике (или зафиксировать отсутствие); посчитать claim-level precision (доля поддержанных утверждений) и citation correctness; оценить, снижает ли RAG/мульти-ретривинг ошибки или иногда ухудшает их (из-за шума в ретривере). Учесть, что метрики детекции галлюцинаций могут быть нестабильны, поэтому нужен слой ручной валидации на подмножестве. ",
"expected_patterns_by_hypothesis": {
"H1": "LLM+RAG+верификация резко снижает долю неподдержанных утверждений/неверных цитат по сравнению с LLM-only; межзапусковая вариативность ниже.",
"H2": "Основной эффект дает дисциплина формата (структурированное извлечение) и контроль критериев; RAG дает небольшой и неоднородный прирост, возможны деградации на шумных запросах.",
"H3": "Существенная доля неподдержанных утверждений сохраняется даже при RAG; детекторы/авто-проверки дают противоречивые результаты; требуется значимый ручной контроль."
},
"midterm_exam": "Показать на 50–100 статьях, что можно воспроизводимо измерять claim-level поддержку и что вмешательства (RAG/верификация) дают статистически и практически значимый эффект хотя бы на одном домене.",
"final_exam": "Построить «паспорт надежности» пайплайна: какие типы утверждений и какие домены проходят порог (например, ≥0.9 поддержанных ключевых утверждений), где пайплайн запрещен без ручной проверки, и какие настройки минимизируют риск."
}
],
"minimal_publishable_unit": "1) Скопинг-обзор + методический протокол оценки надежности LLM в evidence-review, совместимый с PRISMA 2020 (прозрачные логи, воспроизводимость). 2) Открытый бенчмарк-репозиторий (набор задач: скрининг + claim-level верификация) и сравнительная таблица режимов (LLM-only, structured prompts, RAG, verification, human-in-the-loop) с tradeoff по качеству/стоимости/времени. 3) Практические рекомендации «где можно/нельзя» использовать LLM в обзорах.",
"adjacent_fields_to_scan": [
"Evidence synthesis automation (semi-automated screening, RobotReviewer/ML-assisted SR)",
"Information retrieval & RAG evaluation (качество ретривера, шум, multi-source retrieval)",
"Factuality / hallucination benchmarks и методология измерения галлюцинаций",
"Reproducibility engineering (логирование, контроль версий, экспериментальные протоколы)",
"LLM evaluation (LLM-as-judge, согласованность метрик, robustness)"
],
"keywords_for_search": [
"large language model systematic review screening",
"LLM-assisted citation screening precision recall",
"prompting strategies chain-of-thought few-shot screening abstracts",
"PRISMA 2020 systematic review reporting guideline",
"retrieval augmented generation reduce hallucination factuality",
"citation verification supported claims entailment",
"hallucination taxonomy large language models survey",
"hallucination detection metrics consistency across datasets",
"LLM reproducibility stability temperature prompt sensitivity",
"LLM evidence synthesis automation PRISMA screening"
]
}
}