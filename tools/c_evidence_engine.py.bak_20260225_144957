#!/usr/bin/env python3
# -*- coding: utf-8 -*-
from __future__ import annotations

import argparse, csv, json, math, re, traceback
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Tuple


def now_stamp() -> str:
    return datetime.now().strftime("%Y%m%d_%H%M%S")


def ensure_dir(p: Path) -> None:
    p.mkdir(parents=True, exist_ok=True)


def read_text_utf8_sig(path: Path) -> str:
    return path.read_text(encoding="utf-8-sig", errors="ignore")


def extract_json(text: str) -> dict:
    s = text.strip()
    if s.startswith("{") and s.endswith("}"):
        return json.loads(s)
    m = re.search(r"{[\s\S]*}", s)
    if not m:
        raise ValueError("No JSON object found")
    return json.loads(m.group(0))


def tokenize(text: str) -> List[str]:
    if not text:
        return []
    t = text.lower()
    toks = re.findall(r"[a-zа-яё][a-zа-яё0-9\-]{1,}", t, flags=re.IGNORECASE)
    return [x.strip("-") for x in toks if len(x.strip("-")) >= 3]


# --- stopwords / generic junk (расширено) ---
RU_STOP = set("""
и а но или что это как для при без на в во по из к у о об про над под между с со же ли не да нет
которые который которая которых этим этой этого этих тем теми также
ключевым ключевые ключевая ключевых факторов фактор факторов
данные данных анализа анализ исследование исследования работе работах метод методы модель модели подход подходы
""".split())

EN_STOP = set("""
the and for with from into about this that those these have has had were was are is be been being
study studies result results method methods data analysis analyses using use used based across between within among via
model models approach approaches framework system systems
""".split())

GENERIC_STOP = RU_STOP | EN_STOP | set(["resistance"])  # слово-trap, отдельно обрабатываем


# --- domain vocab (как и раньше, но не будем усложнять) ---
DOMAIN_KWS: Dict[str, List[str]] = {
    "biomed": ["patient","clinical","therapy","treatment","cancer","infection","antibiotic","antimicrobial","microbiome",
               "пациент","клинич","терап","лечен","рак","инфекц","антибиот","антибак","микробиот"],
    "eco_evo": ["species","population","phylog","evolution","ecology","biodiversity","habitat","river","watershed","basin","landscape","riverscape",
                "genetic","genome","wgs","ddrad","radseq","mtdna","introgression","speciation","glacial","fish",
                "вид","популяц","филогеограф","эволюц","эколог","ареал","река","бассейн","ландшафт","генетик","геном","мтднк","рыб","гольян"],
    "social": ["survey","policy","education","econom","market","marketing","psycholog","behavior",
               "опрос","политик","образован","эконом","рынок","маркет","социолог","психолог","поведен"],
    "tech": ["algorithm","machine","learning","neural","optimization","numerical","simulation","mesh","wavelet",
             "алгоритм","машинн","обучен","нейрон","оптимиз","числен","моделирован","сетк","вейвлет"],
    "humanities": ["language","linguistics","morphology","syntax","semantics","history",
                   "язык","лингвист","морфолог","синтакс","семант","истори"],
    "other": []
}

# --- claim triggers: если claim про методы, требуем методные сигналы ---
METHOD_TRIGGERS = {
    "prediction": [
        "out-of-sample","outofsample","oos","cross-validation","crossvalidation","validation","predict","prediction","predictive",
        "accuracy","auc","rmse","generalization","transferability","портируем","переносимость","валидац","кросс","предсказ","точност"
    ],
    "gea": [
        "gea","genotype-environment","genotypeenvironment","environmental association","lfmm","bayenv","rda","db-rda","gradient forest",
        "генотип-среда","ассоциац","сред","lfmm","bayenv","градиент"
    ],
    "riverscape": [
        "riverscape","river network","watershed","basin","stream","dendritic","isolation by resistance","ibr","circuit theory","effective resistance",
        "least-cost","resistance distance","river-distance",
        "риверскейп","река","речной","сеть","бассейн","дренаж","ibr","изол","сопротив"
    ],
    "latent_spatial": [
        "latent","spatial basis","pcnm","mem","moran","eigenvector","gaussian process","mixed model","random effects",
        "латент","пространствен","базис","pcnm","mem","moran","смешан","случайн"
    ],
}

def normalize_title(s: str) -> str:
    s = re.sub(r"<[^>]+>", " ", s or "")
    s = s.lower()
    s = re.sub(r"[^a-zа-яё0-9]+", " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s


def domain_scores(text: str) -> Dict[str, float]:
    toks = set(tokenize(text))
    scores: Dict[str, float] = {k: 0.0 for k in DOMAIN_KWS.keys()}
    if not toks:
        return scores
    for dom, kws in DOMAIN_KWS.items():
        s = 0.0
        for kw in kws:
            if kw in toks:
                s += 1.0
            elif any(kw in t for t in toks):
                s += 0.6
        scores[dom] = s
    return scores


def pick_top_domain(scores: Dict[str, float]) -> Tuple[str, float]:
    items = sorted(scores.items(), key=lambda x: x[1], reverse=True)
    best_dom, best = items[0]
    total = sum(v for _, v in items) or 1.0
    conf = best / total
    if best < 1.0:
        return "other", 0.0
    return best_dom, float(conf)


@dataclass
class Paper:
    openalex_id: str
    doi: str
    title: str
    year: str
    venue: str
    cited_by: int
    abstract: str

    @property
    def text(self) -> str:
        return (self.title or "") + " " + (self.abstract or "")


def load_corpus(path: Path) -> List[Paper]:
    out: List[Paper] = []
    with path.open("r", encoding="utf-8-sig", errors="replace", newline="") as f:
        r = csv.DictReader(f)
        for row in r:
            doi = (row.get("doi") or "").strip()
            if doi.startswith("https://doi.org/"):
                doi = doi.replace("https://doi.org/", "")
            try:
                cb = int(float((row.get("cited_by") or "0").strip() or 0))
            except Exception:
                cb = 0
            out.append(Paper(
                openalex_id=(row.get("openalex_id") or "").strip(),
                doi=doi,
                title=(row.get("title") or "").strip(),
                year=(row.get("year") or "").strip(),
                venue=(row.get("venue") or "").strip(),
                cited_by=cb,
                abstract=(row.get("abstract") or "").strip(),
            ))
    return out


def build_idf(papers: List[Paper]) -> Dict[str, float]:
    df: Dict[str, int] = {}
    n = len(papers)
    for p in papers:
        toks = set(tokenize(p.text))
        for t in toks:
            df[t] = df.get(t, 0) + 1
    idf: Dict[str, float] = {}
    for t, d in df.items():
        idf[t] = math.log((n + 1) / (d + 1)) + 1.0
    return idf


def load_structured_idea(path: Path) -> Dict[str, Any]:
    obj = json.loads(read_text_utf8_sig(path))
    if isinstance(obj, dict) and "structured_idea" in obj:
        return obj
    return {"structured_idea": obj}


def get_field(si: Dict[str, Any], key: str) -> str:
    v = si.get(key)
    if isinstance(v, list):
        return " ".join([str(x) for x in v if str(x).strip()])
    if v is None:
        return ""
    return str(v)


def idea_text_all(st: Dict[str, Any]) -> str:
    si = st.get("structured_idea") or {}
    keys = ["title","problem","background","motivation","main_hypothesis","alternative_hypotheses",
            "key_predictions","claims_to_verify","methods_minimal","data_sources","keywords"]
    parts = []
    for k in keys:
        s = get_field(si, k)
        if s:
            parts.append(s)
    return "\n".join(parts)


def idea_text_for_anchors(st: Dict[str, Any]) -> str:
    si = st.get("structured_idea") or {}
    keys = ["keywords","methods_minimal","data_sources","title"]
    parts = []
    for k in keys:
        s = get_field(si, k)
        if s:
            parts.append(s)
    if not parts:
        parts = [idea_text_all(st)]
    return "\n".join(parts)


def extract_claims(st: Dict[str, Any], max_n: int) -> List[str]:
    si = st.get("structured_idea") or {}
    claims: List[str] = []
    for k in ["claims_to_verify","key_predictions"]:
        v = si.get(k)
        if isinstance(v, list):
            claims.extend([str(x).strip() for x in v if str(x).strip()])
    if not claims:
        for k in ["main_hypothesis","problem"]:
            v = si.get(k)
            if v:
                claims.append(str(v).strip())
    seen=set(); out=[]
    for c in claims:
        c=re.sub(r"\s+"," ",c).strip()
        if not c: continue
        cl=c.lower()
        if cl in seen: continue
        seen.add(cl); out.append(c)
    return out[:max_n]


def clean_anchor_token(t: str) -> bool:
    if not t:
        return False
    if t in GENERIC_STOP:
        return False
    if t.isdigit():
        return False
    return True


def build_anchors(st: Dict[str, Any], idf: Dict[str, float], papers: List[Paper], k: int = 20) -> List[str]:
    raw = idea_text_for_anchors(st)
    toks = tokenize(raw)

    # df fraction filter: выбросить слишком частые слова
    df: Dict[str, int] = {}
    for p in papers:
        for x in set(tokenize(p.text)):
            df[x] = df.get(x, 0) + 1
    n = max(1, len(papers))

    uniq=[]
    seen=set()
    for t in toks:
        if t in seen: 
            continue
        seen.add(t)
        if not clean_anchor_token(t):
            continue
        # если слово встречается в >20% корпуса — это чаще всего слишком общее
        if (df.get(t, 0) / n) > 0.20:
            continue
        uniq.append(t)

    scored = [(idf.get(t, 0.0), t) for t in uniq if idf.get(t, 0.0) > 0]
    scored.sort(key=lambda x: x[0], reverse=True)
    anchors = [t for _, t in scored[:k]]

    # fallback: если всё вычистили, вернём самые редкие из uniq (по df)
    if not anchors:
        uniq2 = sorted(uniq, key=lambda x: df.get(x, 0))
        anchors = uniq2[:k]
    return anchors


def detect_claim_modes(claim: str) -> List[str]:
    c = claim.lower()
    modes=[]
    for name, kws in METHOD_TRIGGERS.items():
        for kw in kws:
            if kw in c:
                modes.append(name)
                break
    # уникализируем
    out=[]
    for m in modes:
        if m not in out:
            out.append(m)
    return out


def count_trigger_hits(text: str, modes: List[str]) -> int:
    if not modes:
        return 0
    t = text.lower()
    hits = 0
    for m in modes:
        for kw in METHOD_TRIGGERS.get(m, []):
            if kw in t:
                hits += 1
    return hits


def rank_papers_for_claim(claim: str, papers: List[Paper], idf: Dict[str, float], anchors: List[str]) -> Tuple[List[Tuple[float, Paper, int, int]], Dict[str, Any]]:
    claim_toks = set([t for t in tokenize(claim) if t not in GENERIC_STOP])
    modes = detect_claim_modes(claim)
    qc = {"claim_modes": modes, "method_signal_in_corpus": 0}

    ranked: List[Tuple[float, Paper, int, int]] = []

    for p in papers:
        ptoks = set(tokenize(p.text))
        inter = claim_toks & ptoks
        base = sum(idf.get(t, 1.0) for t in inter)

        # anchors hit
        ah = 0
        if anchors:
            txt = " ".join(ptoks)
            ah = sum(1 for a in anchors if a in txt)

        # method hits by claim modes
        mh = count_trigger_hits(p.text, modes)
        if mh > 0:
            qc["method_signal_in_corpus"] += 1

        score = base + ah * 1.1 + mh * 2.2

        # penalty if claim is method-heavy but paper has no method signal
        if modes and mh == 0:
            score *= 0.25  # резко, иначе снова будет обычная филогеография

        # small citation bonus
        if p.cited_by > 0:
            score *= (1.0 + min(0.35, math.log10(p.cited_by + 1) / 6.0))

        if score > 0:
            ranked.append((score, p, ah, mh))

    ranked.sort(key=lambda x: x[0], reverse=True)
    return ranked, qc


def dedup_papers(papers: List[Paper]) -> List[Paper]:
    # Дедуп по нормализованному title (в т.ч. случаи “одна статья с разными DOI”)
    best: Dict[str, Paper] = {}
    for p in papers:
        key = normalize_title(p.title) or (p.doi.lower() if p.doi else "") or p.openalex_id
        if not key:
            continue
        cur = best.get(key)
        if cur is None:
            best[key] = p
        else:
            # выбрать “лучший”: DOI есть / cited_by выше / abstract есть
            cur_score = (1 if cur.doi else 0, cur.cited_by, 1 if cur.abstract else 0)
            p_score   = (1 if p.doi else 0, p.cited_by, 1 if p.abstract else 0)
            if p_score > cur_score:
                best[key] = p
    return list(best.values())


def validate_llm(obj: Dict[str, Any]) -> Tuple[bool, str, List[Dict[str, Any]]]:
    if not isinstance(obj, dict):
        return False, "Response is not a JSON object.", []
    rows = obj.get("evidence_rows")
    if isinstance(rows, list):
        flat = [x for x in rows if isinstance(x, dict)]
        if not flat:
            return False, "evidence_rows is empty.", []
        return True, "OK", flat
    return False, "Missing evidence_rows.", []


def write_csv(path: Path, rows: List[Dict[str, Any]]) -> None:
    cols=["claim_id","claim","source_id","doi","title","year","venue","openalex_id","relation","quote","quote_location","certainty","certainty_reason"]
    with path.open("w", newline="", encoding="utf-8") as f:
        w=csv.DictWriter(f, fieldnames=cols)
        w.writeheader()
        for r in rows:
            w.writerow({c:r.get(c,"") for c in cols})


def write_md(path: Path, claims: List[str], rows: List[Dict[str, Any]], qc: Dict[str, Any]) -> None:
    by={}
    for r in rows:
        try: cid=int(str(r.get("claim_id","")).strip())
        except: continue
        by.setdefault(cid, []).append(r)

    lines=[]
    lines.append("# Сводка доказательств (Stage C)\n")
    lines.append(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    lines.append("## QC и Screening (автоматически)\n")

    for k in ["mode","papers_total","kept_after_screening","no_abstract_pct","idea_domains","idea_conf","corpus_domain","corpus_conf",
              "domain_counts","allowed_domains","anchors","dropped","claim_qc","gap_warnings"]:
        if k in qc:
            lines.append(f"- {k}: {qc[k]}")
    lines.append("")

    for i,c in enumerate(claims,1):
        rr=by.get(i,[])
        sup=sum(1 for x in rr if str(x.get("relation","")).lower()=="supports")
        con=sum(1 for x in rr if str(x.get("relation","")).lower()=="contradicts")
        unc=sum(1 for x in rr if str(x.get("relation","")).lower()=="unclear")
        lines.append(f"## Утверждение {i}\n{c}\n")
        lines.append(f"- sources: {len(rr)} (supports={sup}, contradicts={con}, unclear={unc})\n")

    path.write_text("\n".join(lines), encoding="utf-8")


def main() -> int:
    ap=argparse.ArgumentParser()
    ap.add_argument("--idea", required=True)
    ap.add_argument("--mode", default="fast", choices=["fast","deep"])
    ap.add_argument("--claims-max", type=int, default=8)
    ap.add_argument("--k", type=int, default=0)
    ap.add_argument("--no-llm", action="store_true")
    ap.add_argument("--prompt-max-abs", type=int, default=1200)
    args=ap.parse_args()

    mode = args.mode
    k = args.k if args.k > 0 else (6 if mode == "fast" else 18)

    idea_dir=Path(args.idea)
    in_dir, out_dir, logs_dir = idea_dir/"in", idea_dir/"out", idea_dir/"logs"
    ensure_dir(in_dir); ensure_dir(out_dir); ensure_dir(logs_dir)

    log_path=logs_dir/f"moduleC_{now_stamp()}.log"
    def log(s:str)->None:
        with log_path.open("a", encoding="utf-8") as f:
            f.write(s+"\n")

    try:
        structured=out_dir/"structured_idea.json"
        corpus=out_dir/"corpus.csv"
        if not structured.exists():
            log("[ERROR] Missing out/structured_idea.json"); return 1
        if not corpus.exists():
            log("[ERROR] Missing out/corpus.csv"); return 1

        st=load_structured_idea(structured)
        idea_text=idea_text_all(st)
        claims=extract_claims(st, max_n=max(1,int(args.claims_max)))
        if not claims:
            log("[ERROR] Could not extract claims"); return 1

        papers_all=dedup_papers(load_corpus(corpus))
        if not papers_all:
            log("[ERROR] corpus.csv empty"); return 1

        # QC domain
        idea_sc = domain_scores(idea_text)
        idea_dom, idea_conf = pick_top_domain(idea_sc)

        counts = {k: 0 for k in DOMAIN_KWS.keys()}
        no_abs = 0
        for p in papers_all:
            if not (p.abstract or "").strip():
                no_abs += 1
            d, _ = pick_top_domain(domain_scores(p.text))
            counts[d] = counts.get(d, 0) + 1
        corpus_dom = max(counts.items(), key=lambda x: x[1])[0]
        corpus_conf = (counts.get(corpus_dom, 0) / max(1, len(papers_all)))

        allowed = [idea_dom] if idea_dom != "other" and idea_conf >= 0.35 else [corpus_dom]

        qc: Dict[str, Any] = {
            "mode": mode,
            "papers_total": len(papers_all),
            "kept_after_screening": 0,
            "no_abstract_pct": round(no_abs / max(1, len(papers_all)), 3),
            "idea_domains": [idea_dom],
            "idea_conf": round(float(idea_conf), 3),
            "corpus_domain": corpus_dom,
            "corpus_conf": round(float(corpus_conf), 3),
            "domain_counts": counts,
            "allowed_domains": allowed,
            "dropped": {"hard_out_of_domain": 0, "soft_no_signal": 0},
        }

        # screen by allowed domain
        screened=[]
        for p in papers_all:
            d, conf = pick_top_domain(domain_scores(p.text))
            if d not in allowed and conf >= 0.60 and d != "other":
                qc["dropped"]["hard_out_of_domain"] += 1
                continue
            screened.append(p)
        qc["kept_after_screening"] = len(screened)

        idf = build_idf(screened if screened else papers_all)
        anchors = build_anchors(st, idf, screened if screened else papers_all, k=20)
        qc["anchors"] = anchors

        candidates=[]
        claim_qc={}
        gap_warnings={}

        total=0
        for i, claim in enumerate(claims,1):
            ranked, cqc = rank_papers_for_claim(claim, screened if screened else papers_all, idf, anchors)

            # gap warning: если claim "методный", но метод-сигналов почти нет
            modes = cqc.get("claim_modes", [])
            if modes and cqc.get("method_signal_in_corpus", 0) == 0:
                gap_warnings[str(i)] = f"В корпусе не найдено статей с метод-терминами для claim_modes={modes}. Скорее всего, Stage B не захватила нужный методический пласт."
            claim_qc[str(i)] = cqc

            top=[]
            used=set()
            # two-pass: сначала берем с method_hits>0 (если claim методный)
            if modes:
                for score,p,ah,mh in ranked:
                    if len(top) >= k: break
                    key = normalize_title(p.title) or p.doi or p.openalex_id
                    if key in used: continue
                    if mh <= 0: 
                        continue
                    used.add(key)
                    abs_clean=re.sub(r"\s+"," ",(p.abstract or "")).strip()
                    if abs_clean and len(abs_clean) > int(args.prompt_max_abs):
                        abs_clean = abs_clean[:int(args.prompt_max_abs)]+"…"
                    if abs_clean:
                        text_for_llm=abs_clean; quote_hint="abstract"
                    else:
                        text_for_llm=f"TITLE_ONLY: {p.title}"; quote_hint="title"
                    dom, conf = pick_top_domain(domain_scores(p.text))
                    top.append({
                        "source_id": f"S{len(top)+1}",
                        "score": round(float(score),4),
                        "anchor_hits": int(ah),
                        "method_hits": int(mh),
                        "paper_domain": dom,
                        "paper_domain_conf": round(float(conf),3),
                        "openalex_id": p.openalex_id,
                        "doi": p.doi,
                        "title": p.title,
                        "year": p.year,
                        "venue": p.venue,
                        "cited_by": p.cited_by,
                        "abstract": abs_clean,
                        "text_for_llm": text_for_llm,
                        "quote_hint": quote_hint,
                        "weak_match": False
                    })

            # добивка до k (если не хватило)
            for score,p,ah,mh in ranked:
                if len(top) >= k: break
                key = normalize_title(p.title) or p.doi or p.openalex_id
                if key in used: continue
                used.add(key)
                abs_clean=re.sub(r"\s+"," ",(p.abstract or "")).strip()
                if abs_clean and len(abs_clean) > int(args.prompt_max_abs):
                    abs_clean = abs_clean[:int(args.prompt_max_abs)]+"…"
                if abs_clean:
                    text_for_llm=abs_clean; quote_hint="abstract"
                else:
                    text_for_llm=f"TITLE_ONLY: {p.title}"; quote_hint="title"
                dom, conf = pick_top_domain(domain_scores(p.text))
                top.append({
                    "source_id": f"S{len(top)+1}",
                    "score": round(float(score),4),
                    "anchor_hits": int(ah),
                    "method_hits": int(mh),
                    "paper_domain": dom,
                    "paper_domain_conf": round(float(conf),3),
                    "openalex_id": p.openalex_id,
                    "doi": p.doi,
                    "title": p.title,
                    "year": p.year,
                    "venue": p.venue,
                    "cited_by": p.cited_by,
                    "abstract": abs_clean,
                    "text_for_llm": text_for_llm,
                    "quote_hint": quote_hint,
                    "weak_match": True if modes and mh == 0 else False
                })

            total += len(top)
            candidates.append({"claim_id": i, "claim": claim, "sources": top})

        qc["claim_qc"] = claim_qc
        qc["gap_warnings"] = gap_warnings

        out_dir.joinpath("evidence_candidates.json").write_text(
            json.dumps({"claims": candidates, "qc": qc}, ensure_ascii=False, indent=2),
            encoding="utf-8"
        )

        if total == 0:
            log("[ERROR] No candidates found."); return 1

        # LLM handoff
        llm_path = in_dir/"llm_evidence.json"
        prompt_path = out_dir/"llm_prompt_C.txt"

        if args.no_llm:
            rows=[]
            for c in candidates:
                for s in c["sources"]:
                    rows.append({
                        "claim_id": c["claim_id"], "claim": c["claim"],
                        "source_id": s["source_id"], "doi": s.get("doi",""), "title": s.get("title",""),
                        "year": s.get("year",""), "venue": s.get("venue",""), "openalex_id": s.get("openalex_id",""),
                        "relation": "unclear", "quote": (s.get("abstract") or s.get("title") or "")[:260],
                        "quote_location": "abstract|title", "certainty": "Low", "certainty_reason": "no_llm_mode"
                    })
            write_csv(out_dir/"evidence_table.csv", rows)
            write_md(out_dir/"evidence_summary.md", claims, rows, qc)
            log("[OK] no-llm complete"); return 0

        if not llm_path.exists():
            llm_path.write_text('{"paste":"Paste ChatGPT JSON here (replace this file)"}\n', encoding="utf-8")

        raw = read_text_utf8_sig(llm_path)
        try:
            obj = extract_json(raw) if raw.strip() else {"paste":"Paste ChatGPT JSON here"}
        except Exception:
            obj = {"paste":"Paste ChatGPT JSON here"}

        ok, why, flat = validate_llm(obj)
        if not ok:
            tpl = Path(__file__).resolve().parents[1]/"config"/"prompts"/"llm_moduleC_prompt.txt"
            packet = json.dumps({"claims": candidates, "qc": qc}, ensure_ascii=False, indent=2)
            prompt = read_text_utf8_sig(tpl).replace("{{CANDIDATES_JSON}}", packet)
            prompt_path.write_text(prompt, encoding="utf-8")
            llm_path.write_text(json.dumps({"paste":"Paste ChatGPT JSON here (replace this file)","reason_pipeline_waiting":why}, ensure_ascii=False, indent=2), encoding="utf-8")
            log("[NEED] LLM: " + why)
            return 2

        # merge back
        cand_map={}
        claim_text={}
        for c in candidates:
            cid=int(c["claim_id"])
            claim_text[cid]=c["claim"]
            cand_map[cid]={s["source_id"]: s for s in c["sources"]}

        out_rows=[]
        seen=set()
        for r in flat:
            try: cid=int(str(r.get("claim_id","")).strip())
            except: continue
            sid=str(r.get("source_id","")).strip()
            if not sid or (cid,sid) in seen:
                continue
            seen.add((cid,sid))
            meta = cand_map.get(cid,{}).get(sid)
            if not meta:
                continue
            rel=str(r.get("relation","unclear")).lower().strip()
            if rel not in ("supports","contradicts","unclear"):
                rel="unclear"
            cert=str(r.get("certainty","Low")).strip()
            if cert not in ("High","Med","Low"):
                cert="Low"
            quote=(r.get("quote") or meta.get("abstract") or meta.get("title") or "")[:260]
            out_rows.append({
                "claim_id": cid, "claim": claim_text.get(cid,""),
                "source_id": sid, "doi": meta.get("doi",""), "title": meta.get("title",""),
                "year": meta.get("year",""), "venue": meta.get("venue",""), "openalex_id": meta.get("openalex_id",""),
                "relation": rel, "quote": quote, "quote_location": (r.get("quote_location") or meta.get("quote_hint") or "abstract"),
                "certainty": cert, "certainty_reason": (r.get("certainty_reason") or "").strip()
            })

        if not out_rows:
            log("[ERROR] LLM JSON parsed but no rows matched."); return 1

        write_csv(out_dir/"evidence_table.csv", out_rows)
        write_md(out_dir/"evidence_summary.md", claims, out_rows, qc)
        log("[OK] Module C complete"); return 0

    except Exception as e:
        log("[ERROR] " + repr(e))
        log(traceback.format_exc())
        return 1


if __name__ == "__main__":
    raise SystemExit(main())