#!/usr/bin/env python3
# -*- coding: utf-8 -*-
from __future__ import annotations

import argparse, csv, json, math, re, traceback
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple


def now_stamp() -> str:
    return datetime.now().strftime("%Y%m%d_%H%M%S")


def ensure_dir(p: Path) -> None:
    p.mkdir(parents=True, exist_ok=True)


def read_text_utf8_sig(path: Path) -> str:
    return path.read_text(encoding="utf-8-sig", errors="ignore")


def extract_json(text: str) -> dict:
    s = text.strip()
    if s.startswith("{") and s.endswith("}"):
        return json.loads(s)
    m = re.search(r"{[\s\S]*}", s)
    if not m:
        raise ValueError("No JSON object found")
    return json.loads(m.group(0))


def is_placeholder(obj: dict) -> bool:
    if not isinstance(obj, dict):
        return True
    keys = set(obj.keys())
    if keys.issubset({"paste", "note", "reason_pipeline_waiting"}):
        return True
    if "paste" in obj and "Paste ChatGPT JSON" in str(obj.get("paste", "")):
        return True
    return False


def tokenize(text: str) -> List[str]:
    if not text:
        return []
    t = text.lower()
    toks = re.findall(r"[a-zа-яё][a-zа-яё0-9\-]{1,}", t, flags=re.IGNORECASE)
    stop = {
        # EN
        "the","and","for","with","from","into","about","this","that","those","these","have","has","had",
        "were","was","are","is","be","been","being","study","studies","result","results","method","methods",
        "data","analysis","analyses","using","use","used","based","across","between","within","among","via",
        "new","novel","approach","model","models","system","systems","framework",
        # RU
        "это","как","что","для","при","без","или","и","а","но","на","в","во","по","из","к","у","о","об","про",
        "над","под","между","с","со","же","ли","не","да","нет","работа","исследование","анализ","данные","метод",
        "методы","модель","модели","подход","подходы","система","системы","результат","результаты",
    }
    out: List[str] = []
    for x in toks:
        x = x.strip("-")
        if len(x) < 3:
            continue
        if x in stop:
            continue
        out.append(x)
    return out


# --- Domain keyword sets (expanded to avoid "other=everything") ---
DOMAIN_KWS: Dict[str, List[str]] = {
    "biomed": [
        "patient","patients","clinical","therapy","treatment","tumor","cancer","carcinoma","disease","infection",
        "microbiome","microbiota","antibiotic","antimicrobial","drug","bacteria","cell","cells","human","humans",
        "пациент","клинич","терап","лечен","опухол","рак","болезн","инфекц","микробиот","антибиот","антибак","клетк","человек",
    ],
    "eco_evo": [
        "species","population","populations","phylog","phylogeography","evolution","ecology","biodiversity","habitat",
        "river","rivers","stream","watershed","basin","landscape","riverscape","connectivity","gene","genes","genetic",
        "genomic","genome","wgs","ddrad","radseq","mtdna","mitochond","introgression","speciation","glacial","ibd","ibr",
        "fish","fishes","minnow","salmon","trout",
        "вид","популяц","филогеограф","эволюц","эколог","биоразнообраз","ареал","река","ручей","бассейн","ландшафт","риверскейп",
        "связност","генетик","геном","вгс","мтднк","интрогресс","видообраз","рыб","гольян",
    ],
    "social": [
        "survey","questionnaire","policy","governance","education","econom","economics","market","marketing",
        "social","sociolog","psycholog","behavior","behaviour","attitude",
        "climate policy","emissions","carbon","mitigation",
        "опрос","политик","управлен","образован","эконом","рынок","маркет","социолог","психолог","поведен",
        "выброс","парников","углерод","митигац","смягчен","климатическ","политик",
    ],
    "tech": [
        "algorithm","algorithms","machine","learning","neural","network","computer","computing","software","hardware",
        "sensor","optimization","optimisation","numerical","simulation","finite","mesh","wavelet","computational",
        "fluid","gas dynamics","aerodynamics",
        "алгоритм","машинн","обучен","нейрон","компьют","софт","железо","датчик","оптимиз","числен",
        "моделирован","симуляц","сетк","декартов","вейвлет","вычислител","газов","динамик",
    ],
    "humanities": [
        "linguistics","language","languages","morphology","syntax","semantics","philology","history","historical",
        "philosophy","literature",
        "лингвист","язык","языки","морфолог","синтакс","семант","филолог","истори","философ","литератур",
    ],
    "other": []
}

GENERIC_ANCHOR_STOP = set([
    # RU generic
    "адаптация","адаптации","адаптивный","адаптивных","адаптировать","локальной","локальная","баланс","решения","сценариев",
    "главная","большинстве","значительная","учет","учёт","число","одной","даже","всего","структуры","зависимость",
    "переносимость","точность","ассоциации","факторы","основе","влияния","данным","подход",
    # EN generic
    "adaptation","adaptive","model","models","framework","approach","strategy","strategies","policy","analysis","method","methods",
    "dependent","background","out-of-sample","oos",
    # trap words that cause drift when not specific
    "resistance"
])

# Domain "core" terms: used for stricter eligibility
DOMAIN_CORE: Dict[str, List[str]] = {
    "eco_evo": ["population","species","gene","genetic","genome","phylog","ecolog","biodivers","river","landscape","riverscape","fish",
                "популяц","вид","генет","геном","эколог","река","ландшафт","рыб"],
    "biomed": ["patient","clinical","therapy","disease","infection","cell","cancer","antibiotic","microbiome",
               "пациент","клинич","терап","болезн","инфекц","клетк","рак","антибиот","микробиот"],
    "social": ["survey","policy","education","econom","market","governance","emissions","carbon","mitigation",
               "опрос","политик","образован","эконом","рынок","управлен","выброс","углерод","митигац"],
    "tech": ["algorithm","optimization","numerical","simulation","mesh","wavelet","software","hardware",
             "алгоритм","оптимиз","числен","моделирован","сетк","вейвлет","софт","компьют"],
    "humanities": ["language","linguistics","morphology","syntax","semantics","history","literature",
                   "язык","лингвист","морфолог","синтакс","семант","истори","литератур"],
    "other": []
}


def domain_scores(text: str) -> Dict[str, float]:
    toks = set(tokenize(text))
    scores = {k: 0.0 for k in DOMAIN_KWS.keys()}
    if not toks:
        return scores
    for dom, kws in DOMAIN_KWS.items():
        s = 0.0
        for kw in kws:
            # allow substring-style matches for stems
            if kw in toks:
                s += 1.0
            elif any(kw in t for t in toks):
                s += 0.6
        scores[dom] = s
    return scores


def pick_domains(scores: Dict[str, float]) -> Tuple[List[str], float]:
    items = sorted(scores.items(), key=lambda x: x[1], reverse=True)
    best_dom, best = items[0]
    total = sum(v for _, v in items) or 1.0
    conf = best / total
    allowed = [best_dom]
    if len(items) > 1:
        second_dom, second = items[1]
        if second > 0 and (second / (best + 1e-9)) >= 0.75:
            allowed.append(second_dom)
    if best < 1.0:
        return ["other"], 0.0
    return allowed, float(conf)


@dataclass
class Paper:
    openalex_id: str
    doi: str
    title: str
    year: str
    venue: str
    cited_by: int
    abstract: str

    @property
    def key(self) -> str:
        return self.doi or self.openalex_id or self.title

    @property
    def text(self) -> str:
        return (self.title or "") + " " + (self.abstract or "")


def load_corpus(path: Path) -> List[Paper]:
    out: List[Paper] = []
    with path.open("r", encoding="utf-8-sig", errors="replace", newline="") as f:
        r = csv.DictReader(f)
        for row in r:
            doi = (row.get("doi") or "").strip()
            if doi.startswith("https://doi.org/"):
                doi = doi.replace("https://doi.org/", "")
            try:
                cb = int(float((row.get("cited_by") or "0").strip() or 0))
            except Exception:
                cb = 0
            out.append(Paper(
                openalex_id=(row.get("openalex_id") or "").strip(),
                doi=doi,
                title=(row.get("title") or "").strip(),
                year=(row.get("year") or "").strip(),
                venue=(row.get("venue") or "").strip(),
                cited_by=cb,
                abstract=(row.get("abstract") or "").strip(),
            ))
    return out


def build_idf(papers: List[Paper]) -> Dict[str, float]:
    df: Dict[str, int] = {}
    n = len(papers)
    for p in papers:
        toks = set(tokenize(p.text))
        for t in toks:
            df[t] = df.get(t, 0) + 1
    idf: Dict[str, float] = {}
    for t, d in df.items():
        idf[t] = math.log((n + 1) / (d + 1)) + 1.0
    return idf


def load_structured_idea(path: Path) -> Dict[str, Any]:
    obj = json.loads(read_text_utf8_sig(path))
    if isinstance(obj, dict) and "structured_idea" in obj:
        return obj
    return {"structured_idea": obj}


def get_field(si: Dict[str, Any], key: str) -> str:
    v = si.get(key)
    if isinstance(v, list):
        return " ".join([str(x) for x in v if str(x).strip()])
    if v is None:
        return ""
    return str(v)


def idea_text_all(st: Dict[str, Any]) -> str:
    si = st.get("structured_idea") or {}
    keys = ["title","problem","background","motivation","main_hypothesis","alternative_hypotheses",
            "key_predictions","claims_to_verify","methods_minimal","data_sources","keywords"]
    parts = []
    for k in keys:
        s = get_field(si, k)
        if s:
            parts.append(s)
    return "\n".join(parts)


def idea_text_for_anchors(st: Dict[str, Any]) -> str:
    # Prefer more specific fields first (универсально)
    si = st.get("structured_idea") or {}
    keys = ["keywords","methods_minimal","data_sources","title"]
    parts = []
    for k in keys:
        s = get_field(si, k)
        if s:
            parts.append(s)
    # fallback to whole idea
    if not parts:
        parts = [idea_text_all(st)]
    return "\n".join(parts)


def extract_claims(st: Dict[str, Any], max_n: int) -> List[str]:
    si = st.get("structured_idea") or {}
    claims: List[str] = []
    for k in ["claims_to_verify","key_predictions"]:
        v = si.get(k)
        if isinstance(v, list):
            claims.extend([str(x).strip() for x in v if str(x).strip()])
    if not claims:
        for k in ["main_hypothesis","problem"]:
            v = si.get(k)
            if v:
                claims.append(str(v).strip())
        alts = si.get("alternative_hypotheses")
        if isinstance(alts, list):
            claims.extend([str(x).strip() for x in alts if str(x).strip()])
    seen=set(); out=[]
    for c in claims:
        c=re.sub(r"\s+"," ",c).strip()
        if not c: continue
        cl=c.lower()
        if cl in seen: continue
        seen.add(cl); out.append(c)
    return out[:max_n]


def paper_domain(p: Paper) -> Tuple[str, float, Dict[str, float]]:
    sc = domain_scores(p.text)
    allowed, conf = pick_domains(sc)
    return allowed[0], conf, sc


def estimate_qc(papers: List[Paper], st: Dict[str, Any]) -> Dict[str, Any]:
    idea_sc = domain_scores(idea_text_all(st))
    idea_domains, idea_conf = pick_domains(idea_sc)

    counts = {k: 0 for k in DOMAIN_KWS.keys()}
    no_abs = 0
    for p in papers:
        if not (p.abstract or "").strip():
            no_abs += 1
        d, _, _ = paper_domain(p)
        counts[d] = counts.get(d, 0) + 1

    corpus_top = max(counts.items(), key=lambda x: x[1])[0] if papers else "other"
    corpus_conf = (counts.get(corpus_top, 0) / max(1, len(papers))) if papers else 0.0

    return {
        "papers_total": len(papers),
        "no_abstract_papers": no_abs,
        "no_abstract_pct": round(no_abs / max(1, len(papers)), 3),
        "idea_domains": idea_domains,
        "idea_conf": round(float(idea_conf), 3),
        "corpus_domain": corpus_top,
        "corpus_conf": round(float(corpus_conf), 3),
        "domain_counts": counts,
    }


def allowed_domains_from_qc(qc: Dict[str, Any]) -> List[str]:
    idea_domains = qc.get("idea_domains") or ["other"]
    idea_conf = float(qc.get("idea_conf", 0.0))
    if idea_domains and idea_domains != ["other"] and idea_conf >= 0.35:
        return list(dict.fromkeys(idea_domains))
    cd = qc.get("corpus_domain") or "other"
    return [cd]


def build_specific_anchors(st: Dict[str, Any], idf_global: Dict[str, float], k: int = 25) -> List[str]:
    raw = idea_text_for_anchors(st)
    toks = tokenize(raw)

    uniq=[]
    seen=set()
    for t in toks:
        if t in seen: 
            continue
        seen.add(t)
        uniq.append(t)

    scored=[]
    for t in uniq:
        if t in GENERIC_ANCHOR_STOP:
            continue
        score = idf_global.get(t, 0.0)
        if score <= 0:
            continue
        scored.append((score, t))

    scored.sort(key=lambda x: x[0], reverse=True)
    anchors=[t for s,t in scored[:k]]

    # If anchors are empty (rare), fall back to top-idf tokens not generic
    if not anchors:
        candidates = [(idf_global.get(t,0.0), t) for t in uniq if t not in GENERIC_ANCHOR_STOP]
        candidates.sort(key=lambda x:x[0], reverse=True)
        anchors = [t for s,t in candidates[:k] if s > 0]

    return anchors


def has_core_terms(p: Paper, allowed_domains: List[str]) -> bool:
    txt = " ".join(tokenize(p.text))
    for d in allowed_domains:
        for kw in DOMAIN_CORE.get(d, []):
            if kw in txt:
                return True
    return False


def count_anchor_hits(p: Paper, anchors: List[str]) -> int:
    if not anchors:
        return 0
    txt = " ".join(tokenize(p.text))
    return sum(1 for a in anchors if a in txt)


def screen_papers(papers: List[Paper], allowed_domains: List[str], anchors: List[str]) -> Tuple[List[Paper], Dict[str, int]]:
    kept: List[Paper] = []
    dropped = {"hard_out_of_domain": 0, "soft_no_signal": 0}

    for p in papers:
        dom, conf, sc = paper_domain(p)

        # Hard drop: confident non-allowed domain
        if dom not in allowed_domains and conf >= 0.55 and dom != "other":
            dropped["hard_out_of_domain"] += 1
            continue

        # Compute signals
        ah = count_anchor_hits(p, anchors)
        core = has_core_terms(p, allowed_domains)

        # Soft drop: no core terms and no anchors, and paper strongly belongs to another domain
        if (not core) and (ah == 0):
            # if it has any noticeable score in a non-allowed domain -> drop
            non_allowed_peak = 0.0
            for d, v in sc.items():
                if d not in allowed_domains and d != "other":
                    non_allowed_peak = max(non_allowed_peak, v)
            if non_allowed_peak >= 1.4:
                dropped["soft_no_signal"] += 1
                continue

        kept.append(p)

    return kept, dropped


def build_idf_for_tokens(papers: List[Paper]) -> Dict[str, float]:
    return build_idf(papers)


def mmr_select(ranked: List[Tuple[float, Paper, int]], k: int, lam: float = 0.75) -> List[Tuple[float, Paper, int]]:
    # Maximal Marginal Relevance with Jaccard on token sets
    if not ranked:
        return []
    selected: List[Tuple[float, Paper, int]] = []
    selected_sets: List[set] = []

    def tokset(p: Paper) -> set:
        return set(tokenize(p.title + " " + (p.abstract or "")))

    while ranked and len(selected) < k:
        if not selected:
            s = ranked.pop(0)
            selected.append(s)
            selected_sets.append(tokset(s[1]))
            continue

        best_i = 0
        best_val = -1e9
        for i, (score, p, ah) in enumerate(ranked[:200]):  # limit for speed
            S = tokset(p)
            max_sim = 0.0
            for SS in selected_sets:
                inter = len(S & SS)
                uni = len(S | SS) or 1
                sim = inter / uni
                if sim > max_sim:
                    max_sim = sim
            val = lam * score - (1 - lam) * (max_sim * 10.0)
            if val > best_val:
                best_val = val
                best_i = i

        s = ranked.pop(best_i)
        selected.append(s)
        selected_sets.append(tokset(s[1]))

    return selected


def rank_for_claim(claim: str, papers: List[Paper], idf: Dict[str, float], anchors: List[str]) -> List[Tuple[float, Paper, int]]:
    ct = set(tokenize(claim))
    ranked=[]
    for p in papers:
        toks = set(tokenize(p.text))
        inter = ct & toks
        base = sum(idf.get(t, 1.0) for t in inter)
        ah = count_anchor_hits(p, anchors)
        score = base + ah * 1.2  # anchors matter more now

        if p.cited_by > 0:
            score *= (1.0 + min(0.35, math.log10(p.cited_by + 1) / 6.0))

        if score > 0:
            ranked.append((score, p, ah))
    ranked.sort(key=lambda x:x[0], reverse=True)
    return ranked


def validate_llm(obj: Dict[str, Any]) -> Tuple[bool, str, List[Dict[str, Any]]]:
    if not isinstance(obj, dict):
        return False, "Response is not a JSON object.", []
    if is_placeholder(obj):
        return False, "Placeholder JSON (paste ChatGPT output).", []
    rows = obj.get("evidence_rows")
    if isinstance(rows, list):
        flat = [x for x in rows if isinstance(x, dict)]
        if not flat:
            return False, "evidence_rows is empty.", []
        return True, "OK", flat
    return False, "Missing evidence_rows.", []


def write_table_csv(path: Path, rows: List[Dict[str, Any]]) -> None:
    cols=["claim_id","claim","source_id","doi","title","year","venue","openalex_id","relation","quote","quote_location","certainty","certainty_reason"]
    with path.open("w", newline="", encoding="utf-8") as f:
        w=csv.DictWriter(f, fieldnames=cols)
        w.writeheader()
        for r in rows:
            w.writerow({c:r.get(c,"") for c in cols})


def write_summary_md(path: Path, claims: List[str], rows: List[Dict[str, Any]], qc: Dict[str, Any]) -> None:
    by={}
    for r in rows:
        try: cid=int(str(r.get("claim_id","")).strip())
        except: continue
        by.setdefault(cid, []).append(r)

    lines=[]
    lines.append("# Сводка доказательств (Stage C)\n")
    lines.append(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")

    lines.append("## QC и Screening (автоматически)\n")
    for k in ["mode","papers_total","kept_after_screening","no_abstract_pct","idea_domains","idea_conf","corpus_domain","corpus_conf","domain_counts","allowed_domains","anchors","dropped"]:
        if k in qc:
            lines.append(f"- {k}: {qc[k]}")
    lines.append("")

    for i,c in enumerate(claims,1):
        rr=by.get(i,[])
        sup=sum(1 for x in rr if str(x.get("relation","")).lower()=="supports")
        con=sum(1 for x in rr if str(x.get("relation","")).lower()=="contradicts")
        unc=sum(1 for x in rr if str(x.get("relation","")).lower()=="unclear")
        lines.append(f"## Утверждение {i}\n{c}\n")
        lines.append(f"- sources: {len(rr)} (supports={sup}, contradicts={con}, unclear={unc})\n")
    path.write_text("\n".join(lines), encoding="utf-8")


def main() -> int:
    ap=argparse.ArgumentParser()
    ap.add_argument("--idea", required=True)
    ap.add_argument("--mode", default="fast", choices=["fast","deep"])
    ap.add_argument("--claims-max", type=int, default=8)
    ap.add_argument("--k", type=int, default=0)
    ap.add_argument("--no-llm", action="store_true")
    ap.add_argument("--prompt-max-abs", type=int, default=1200)
    args=ap.parse_args()

    mode = args.mode
    k = args.k
    if k <= 0:
        k = 6 if mode == "fast" else 18

    idea_dir=Path(args.idea)
    in_dir, out_dir, logs_dir = idea_dir/"in", idea_dir/"out", idea_dir/"logs"
    ensure_dir(in_dir); ensure_dir(out_dir); ensure_dir(logs_dir)

    log_path=logs_dir/f"moduleC_{now_stamp()}.log"
    def log(s:str)->None:
        with log_path.open("a", encoding="utf-8") as f:
            f.write(s+"\n")

    try:
        structured=out_dir/"structured_idea.json"
        corpus=out_dir/"corpus.csv"
        if not structured.exists():
            log("[ERROR] Missing out/structured_idea.json"); return 1
        if not corpus.exists():
            log("[ERROR] Missing out/corpus.csv"); return 1

        st=load_structured_idea(structured)
        claims=extract_claims(st, max_n=max(1,int(args.claims_max)))
        if not claims:
            log("[ERROR] Could not extract claims"); return 1

        papers_all=load_corpus(corpus)
        if not papers_all:
            log("[ERROR] corpus.csv empty"); return 1

        qc=estimate_qc(papers_all, st)
        allowed=allowed_domains_from_qc(qc)
        qc["allowed_domains"]=allowed
        qc["mode"]=mode

        # Build global idf and specific anchors
        idf_global = build_idf_for_tokens(papers_all)
        anchors = build_specific_anchors(st, idf_global, k=25)
        qc["anchors"]=anchors

        # Screening stricter than domain
        screened, dropped = screen_papers(papers_all, allowed, anchors)
        qc["dropped"]=dropped
        qc["kept_after_screening"]=len(screened)

        # If screening too strict, fallback gently
        if len(screened) < 30:
            log("[QC] WARNING: screening left too few papers; fallback to full corpus for ranking.")
            screened = papers_all

        # IDF for ranking computed on screened set (more precise)
        idf_rank = build_idf_for_tokens(screened)

        candidates=[]
        total=0
        for i, claim in enumerate(claims,1):
            ranked = rank_for_claim(claim, screened, idf_rank, anchors)
            if mode == "deep":
                ranked_sel = mmr_select(ranked, k=k, lam=0.78)
            else:
                ranked_sel = ranked[:k]

            top=[]
            used=set()
            for score, p, ah in ranked_sel:
                if p.key in used:
                    continue
                used.add(p.key)

                abs_clean=re.sub(r"\s+"," ",(p.abstract or "")).strip()
                if abs_clean and len(abs_clean) > int(args.prompt_max_abs):
                    abs_clean = abs_clean[:int(args.prompt_max_abs)]+"…"
                if abs_clean:
                    text_for_llm=abs_clean
                    quote_hint="abstract"
                else:
                    text_for_llm=f"TITLE_ONLY: {p.title}"
                    quote_hint="title"

                dom, conf, _ = paper_domain(p)

                top.append({
                    "source_id": f"S{len(top)+1}",
                    "score": round(float(score),4),
                    "anchor_hits": int(ah),
                    "paper_domain": dom,
                    "paper_domain_conf": round(float(conf),3),
                    "openalex_id": p.openalex_id,
                    "doi": p.doi,
                    "title": p.title,
                    "year": p.year,
                    "venue": p.venue,
                    "cited_by": p.cited_by,
                    "abstract": abs_clean,
                    "text_for_llm": text_for_llm,
                    "quote_hint": quote_hint,
                })

            total += len(top)
            candidates.append({"claim_id": i, "claim": claim, "sources": top})

        out_dir.joinpath("evidence_candidates.json").write_text(
            json.dumps({"claims": candidates, "qc": qc}, ensure_ascii=False, indent=2),
            encoding="utf-8"
        )

        if total == 0:
            log("[ERROR] No candidates found. Possibly corpus is off-topic."); return 1

        # LLM handoff
        llm_path = in_dir/"llm_evidence.json"
        prompt_path = out_dir/"llm_prompt_C.txt"

        if args.no_llm:
            rows=[]
            for c in candidates:
                for s in c["sources"]:
                    rows.append({
                        "claim_id": c["claim_id"], "claim": c["claim"],
                        "source_id": s["source_id"], "doi": s.get("doi",""), "title": s.get("title",""),
                        "year": s.get("year",""), "venue": s.get("venue",""), "openalex_id": s.get("openalex_id",""),
                        "relation": "unclear", "quote": (s.get("abstract") or s.get("title") or "")[:260],
                        "quote_location": "abstract|title", "certainty": "Low", "certainty_reason": "no_llm_mode"
                    })
            write_table_csv(out_dir/"evidence_table.csv", rows)
            write_summary_md(out_dir/"evidence_summary.md", claims, rows, qc)
            log("[OK] no-llm complete"); return 0

        if not llm_path.exists():
            llm_path.write_text('{"paste":"Paste ChatGPT JSON here (replace this file)"}\n', encoding="utf-8")

        raw = read_text_utf8_sig(llm_path)
        try:
            obj = extract_json(raw) if raw.strip() else {"paste":"Paste ChatGPT JSON here"}
        except Exception:
            obj = {"paste":"Paste ChatGPT JSON here"}

        ok, why, flat = validate_llm(obj)
        if not ok:
            tpl = Path(__file__).resolve().parents[1]/"config"/"prompts"/"llm_moduleC_prompt.txt"
            if not tpl.exists():
                log("[ERROR] missing prompt template"); return 1
            packet = json.dumps({"claims": candidates, "qc": qc}, ensure_ascii=False, indent=2)
            prompt = read_text_utf8_sig(tpl).replace("{{CANDIDATES_JSON}}", packet)
            prompt_path.write_text(prompt, encoding="utf-8")
            llm_path.write_text(json.dumps({"paste":"Paste ChatGPT JSON here (replace this file)","reason_pipeline_waiting":why}, ensure_ascii=False, indent=2), encoding="utf-8")
            log("[NEED] LLM: " + why)
            return 2

        # merge back
        cand_map={}
        claim_text={}
        for c in candidates:
            cid=int(c["claim_id"])
            claim_text[cid]=c["claim"]
            cand_map[cid]={s["source_id"]: s for s in c["sources"]}

        out_rows=[]
        seen=set()
        for r in flat:
            try: cid=int(str(r.get("claim_id","")).strip())
            except: continue
            sid=str(r.get("source_id","")).strip()
            if not sid or (cid,sid) in seen:
                continue
            seen.add((cid,sid))
            meta = cand_map.get(cid,{}).get(sid)
            if not meta:
                continue
            rel=str(r.get("relation","unclear")).lower().strip()
            if rel not in ("supports","contradicts","unclear"):
                rel="unclear"
            cert=str(r.get("certainty","Low")).strip()
            if cert not in ("High","Med","Low"):
                cert="Low"
            quote=(r.get("quote") or meta.get("abstract") or meta.get("title") or "")[:260]
            out_rows.append({
                "claim_id": cid, "claim": claim_text.get(cid,""),
                "source_id": sid, "doi": meta.get("doi",""), "title": meta.get("title",""),
                "year": meta.get("year",""), "venue": meta.get("venue",""), "openalex_id": meta.get("openalex_id",""),
                "relation": rel, "quote": quote, "quote_location": (r.get("quote_location") or meta.get("quote_hint") or "abstract"),
                "certainty": cert, "certainty_reason": (r.get("certainty_reason") or "").strip()
            })

        if not out_rows:
            log("[ERROR] LLM JSON parsed but no rows matched."); return 1

        write_table_csv(out_dir/"evidence_table.csv", out_rows)
        write_summary_md(out_dir/"evidence_summary.md", claims, out_rows, qc)
        log("[OK] Module C complete"); return 0

    except Exception as e:
        log("[ERROR] " + repr(e))
        log(traceback.format_exc())
        return 1


if __name__ == "__main__":
    raise SystemExit(main())