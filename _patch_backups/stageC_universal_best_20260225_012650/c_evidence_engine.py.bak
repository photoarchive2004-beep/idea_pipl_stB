#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""Module C (Stage C) — Evidence Table Engine (best-practices hardened)

Simple:
- claims (out/structured_idea.json) + corpus (out/corpus.csv)
- pick relevant sources per claim (topic-guard to reduce off-topic drift)
- generate evidence table via ChatGPT copy/paste (or no-LLM fallback)

Robustness:
- UTF-8 BOM-safe JSON reading (utf-8-sig)
- QC (missing abstracts + domain mismatch warning)
- Topic-guard filtering (helps with ambiguous words like "resistance")
"""

from __future__ import annotations

import argparse
import csv
import json
import math
import re
import traceback
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple


def now_stamp() -> str:
    return datetime.now().strftime("%Y%m%d_%H%M%S")


def ensure_dir(p: Path) -> None:
    p.mkdir(parents=True, exist_ok=True)


def extract_json(text: str) -> dict:
    s = text.strip()
    if s.startswith("{") and s.endswith("}"):
        return json.loads(s)
    m = re.search(r"{[\s\S]*}", s)
    if not m:
        raise ValueError("No JSON object found")
    return json.loads(m.group(0))


def is_placeholder(obj: dict) -> bool:
    keys = set(obj.keys())
    if keys.issubset({"paste", "note", "reason_pipeline_waiting"}):
        return True
    if "paste" in obj and "Paste ChatGPT JSON" in str(obj.get("paste", "")):
        return True
    return False


def tokenize(text: str) -> List[str]:
    if not text:
        return []
    text = text.lower()
    toks = re.findall(r"[a-zа-яё][a-zа-яё0-9\-]{1,}", text, flags=re.IGNORECASE)
    stop = {
        "the","and","for","with","from","into","about","this","that","those","these",
        "have","has","had","were","was","are","is","be","been","being","study","studies",
        "result","results","method","methods","data","analysis","analyses","using","use","used",
        "based","across","between","within","among","via",
        "это","как","что","для","при","без","или","и","а","но","на","в","во","по","из","к",
        "у","о","об","про","над","под","между","с","со","же","ли","не","да","нет",
    }
    out: List[str] = []
    for t in toks:
        t = t.strip("-")
        if len(t) < 3:
            continue
        if t in stop:
            continue
        out.append(t)
    return out


def snippet_around(text: str, tokens: List[str], max_len: int = 260) -> str:
    if not text:
        return ""
    t = re.sub(r"\s+", " ", text).strip()
    if not tokens:
        return t[:max_len]
    low = t.lower()
    idxs = []
    for tok in tokens[:8]:
        i = low.find(tok.lower())
        if i >= 0:
            idxs.append(i)
    if not idxs:
        return t[:max_len]
    i0 = min(idxs)
    start = max(0, i0 - max_len // 2)
    end = min(len(t), start + max_len)
    sn = t[start:end]
    if start > 0:
        sn = "…" + sn
    if end < len(t):
        sn = sn + "…"
    return sn


def read_text_utf8_sig(path: Path) -> str:
    return path.read_text(encoding="utf-8-sig", errors="ignore")


def load_structured_idea(path: Path) -> Dict[str, Any]:
    obj = json.loads(read_text_utf8_sig(path))
    if isinstance(obj, dict) and "structured_idea" in obj:
        return obj
    return {"meta": {"language": "ru"}, "structured_idea": obj}


def extract_claims(obj: Dict[str, Any], max_n: int) -> List[str]:
    si = obj.get("structured_idea") or {}
    claims: List[str] = []

    c0 = si.get("claims_to_verify")
    if isinstance(c0, list):
        claims.extend([str(x).strip() for x in c0 if str(x).strip()])

    kp = si.get("key_predictions")
    if isinstance(kp, list):
        claims.extend([str(x).strip() for x in kp if str(x).strip()])

    if not claims:
        mh = si.get("main_hypothesis")
        if mh:
            claims.append(str(mh).strip())
        alts = si.get("alternative_hypotheses")
        if isinstance(alts, list):
            claims.extend([str(x).strip() for x in alts if str(x).strip()])
        pr = si.get("problem")
        if pr:
            claims.append(str(pr).strip())

    seen = set()
    uniq = []
    for c in claims:
        c = re.sub(r"\s+", " ", c).strip()
        if not c:
            continue
        if c.lower() in seen:
            continue
        seen.add(c.lower())
        uniq.append(c)
    return uniq[:max_n]


def structured_idea_text(obj: Dict[str, Any]) -> str:
    si = obj.get("structured_idea") or {}
    parts: List[str] = []
    for k in [
        "title","problem","background","motivation","main_hypothesis",
        "alternative_hypotheses","key_predictions","claims_to_verify",
        "methods_minimal","data_sources","keywords",
    ]:
        v = si.get(k)
        if isinstance(v, list):
            parts.extend([str(x) for x in v if str(x).strip()])
        elif v:
            parts.append(str(v))
    return "\n".join(parts)


DOMAIN_KEYWORDS: Dict[str, List[str]] = {
    "biomed": [
        "patient","patients","clinical","therapy","treatment","tumor","cancer","carcinoma",
        "microbiome","microbiota","antibiotic","antimicrobial","drug",
        "cell","cells","mouse","mice","human","humans","disease","infection","bacteria",
        "metastasis","immune","immun","her2","rna-seq","mrna","protein","pharmac",
        "карцин","рак","пациент","клинич","терап","лечен","антибиот","антибак",
        "микробиот","инфекц","опухол","клетк",
    ],
    "eco_evo": [
        "species","population","populations","phylogeography","phylogeny","evolution",
        "ecology","biodiversity","habitat","river","rivers","stream","watershed",
        "landscape","riverscape","connectivity","gene","genes","genomic","genome","wgs",
        "ddrad","radseq","mitochond","mtdna","introgression","speciation","glacial",
        "isolation","ibd","ibr","resistance-distance","corridor",
        "fish","fishes","minnow","salmon","trout",
        "вид","популяц","филогеограф","эволюц","эколог","биоразнообраз","ареал",
        "река","речной","озеро","бассейн","ландшафт","риверскейп","генетик","геном",
        "мтднк","интрогресс","видообраз","рыб","гольян",
    ],
    "social": [
        "survey","questionnaire","policy","education","econom","economics","market",
        "governance","social","sociolog","psycholog","behavior","behaviour","attitude",
        "опрос","политик","эконом","рынок","социолог","психолог","поведен",
    ],
    "tech": [
        "algorithm","algorithms","machine","learning","neural","network","networks",
        "computer","computing","software","hardware","sensor","sensors","robot",
        "optimization","optimisation","deep","ai","vision","nlp",
        "алгоритм","машинн","обучен","нейрон","компьют","софт","датчик","робот","оптимиз",
    ],
}


def domain_scores(text: str) -> Dict[str, float]:
    toks = tokenize(text)
    if not toks:
        return {k: 0.0 for k in DOMAIN_KEYWORDS.keys()}
    tokset = set(toks)
    scores: Dict[str, float] = {}
    for dom, kws in DOMAIN_KEYWORDS.items():
        s = 0.0
        for kw in kws:
            if kw in tokset:
                s += 1.0
            else:
                if any(kw in t for t in tokset):
                    s += 0.6
        scores[dom] = s
    return scores


def pick_domain(scores: Dict[str, float]) -> Tuple[str, float]:
    items = list(scores.items())
    if not items:
        return "unknown", 0.0
    best_dom, best = max(items, key=lambda x: x[1])
    total = sum(v for _, v in items)
    if total <= 0:
        return "unknown", 0.0
    return best_dom, float(best / total)


@dataclass
class Paper:
    openalex_id: str
    doi: str
    title: str
    year: str
    venue: str
    cited_by: int
    abstract: str

    @property
    def key(self) -> str:
        return self.doi or self.openalex_id or self.title


def load_corpus(path: Path) -> List[Paper]:
    rows: List[Paper] = []
    with path.open("r", encoding="utf-8-sig", errors="replace", newline="") as f:
        r = csv.DictReader(f)
        for row in r:
            doi = (row.get("doi") or "").strip()
            if doi.startswith("https://doi.org/"):
                doi = doi.replace("https://doi.org/", "")
            cb = 0
            try:
                cb = int(float((row.get("cited_by") or "0").strip() or 0))
            except Exception:
                cb = 0
            rows.append(
                Paper(
                    openalex_id=(row.get("openalex_id") or "").strip(),
                    doi=doi,
                    title=(row.get("title") or "").strip(),
                    year=(row.get("year") or "").strip(),
                    venue=(row.get("venue") or "").strip(),
                    cited_by=cb,
                    abstract=(row.get("abstract") or "").strip(),
                )
            )
    return rows


def build_idf(papers: List[Paper]) -> Dict[str, float]:
    df: Dict[str, int] = {}
    n = len(papers)
    for p in papers:
        toks = set(tokenize((p.title or "") + " " + (p.abstract or "")))
        for t in toks:
            df[t] = df.get(t, 0) + 1
    idf: Dict[str, float] = {}
    for t, d in df.items():
        idf[t] = math.log((n + 1) / (d + 1)) + 1.0
    return idf


def paper_domain(p: Paper) -> Tuple[str, float]:
    sc = domain_scores((p.title or "") + " " + (p.abstract or ""))
    return pick_domain(sc)


def estimate_domains(papers: List[Paper], idea_text: str) -> Dict[str, Any]:
    idea_sc = domain_scores(idea_text)
    idea_dom, idea_conf = pick_domain(idea_sc)

    counts: Dict[str, int] = {k: 0 for k in DOMAIN_KEYWORDS.keys()}
    unknown = 0
    no_abs = 0
    for p in papers:
        if not (p.abstract or "").strip():
            no_abs += 1
        dom, conf = paper_domain(p)
        if dom == "unknown" or conf < 0.25:
            unknown += 1
            continue
        counts[dom] = counts.get(dom, 0) + 1

    corpus_dom = "unknown"
    corpus_conf = 0.0
    if sum(counts.values()) > 0:
        corpus_dom = max(counts.items(), key=lambda x: x[1])[0]
        corpus_conf = counts[corpus_dom] / max(1, sum(counts.values()))

    return {
        "idea_domain": idea_dom,
        "idea_conf": round(float(idea_conf), 3),
        "corpus_domain": corpus_dom,
        "corpus_conf": round(float(corpus_conf), 3),
        "domain_counts": counts,
        "unknown_papers": unknown,
        "no_abstract_papers": no_abs,
        "no_abstract_pct": round(float(no_abs / max(1, len(papers))), 3),
    }


def choose_target_domain(qc: Dict[str, Any]) -> str:
    idea_dom = qc.get("idea_domain", "unknown")
    idea_conf = float(qc.get("idea_conf", 0.0))
    corpus_dom = qc.get("corpus_domain", "unknown")
    corpus_conf = float(qc.get("corpus_conf", 0.0))

    if idea_dom != "unknown" and idea_conf >= 0.34:
        return idea_dom
    if corpus_dom != "unknown" and corpus_conf >= 0.34:
        return corpus_dom
    return idea_dom if idea_dom != "unknown" else corpus_dom


def topic_guard_filter(papers: List[Paper], target_domain: str) -> List[Paper]:
    if not target_domain or target_domain == "unknown":
        return papers

    out: List[Paper] = []
    for p in papers:
        dom, conf = paper_domain(p)

        if dom == "unknown" or conf < 0.45:
            out.append(p)
            continue

        if dom == target_domain:
            out.append(p)
            continue

        if target_domain == "eco_evo" and dom == "biomed" and conf >= 0.50:
            continue

        out.append(p)
    return out


def penalty_factor(claim: str, p: Paper, target_domain: str) -> float:
    txt = ((p.title or "") + " " + (p.abstract or "")).lower()
    claim_low = (claim or "").lower()

    if target_domain != "biomed":
        trap = ["antimicrobial", "antibiotic", "microbiome", "microbiota", "tumor", "cancer",
                "clinical", "patient", "therapy", "infection", "bacteria", "her2"]
        if any(w in txt for w in trap):
            if any(w in claim_low for w in ["river", "riverscape", "landscape", "fish", "population", "ibr",
                                            "resistance distance", "генет", "река", "гольян", "популяц", "ландшафт"]):
                return 0.25

    if target_domain == "eco_evo":
        if any(w in txt for w in ["clinical", "patient", "cancer", "tumor", "antimicrobial", "antibiotic"]):
            return 0.20

    return 1.0


def rank_papers_for_claim(
    claim: str,
    papers: List[Paper],
    idf: Dict[str, float],
    target_domain: str,
    global_hint_tokens: Optional[List[str]] = None,
) -> List[Tuple[float, Paper]]:
    ct = tokenize(claim)
    expanded: List[str] = list(ct)
    if global_hint_tokens:
        expanded.extend(global_hint_tokens[:18])

    if not expanded:
        return []
    ct_set = set(expanded)

    ranked: List[Tuple[float, Paper]] = []
    for p in papers:
        toks_title = set(tokenize(p.title or ""))
        toks_abs = set(tokenize(p.abstract or ""))
        inter_t = ct_set & toks_title
        inter_a = ct_set & toks_abs
        if not inter_t and not inter_a:
            continue

        score = 0.0
        for t in inter_a:
            score += idf.get(t, 1.0)
        for t in inter_t:
            score += 1.6 * idf.get(t, 1.0)

        if p.cited_by > 0:
            score *= (1.0 + min(0.45, math.log10(p.cited_by + 1) / 5.0))

        score *= penalty_factor(claim, p, target_domain)
        ranked.append((score, p))

    ranked.sort(key=lambda sp: sp[0], reverse=True)
    return ranked


def validate_llm(obj: Dict[str, Any]) -> Tuple[bool, str, List[Dict[str, Any]]]:
    if not isinstance(obj, dict):
        return False, "Response is not a JSON object.", []
    if is_placeholder(obj):
        return False, "Placeholder JSON (paste ChatGPT output).", []

    rows = obj.get("evidence_rows")
    if isinstance(rows, list):
        flat = [x for x in rows if isinstance(x, dict)]
        if not flat:
            return False, "evidence_rows is empty.", []
        return True, "OK", flat

    return False, "Missing evidence_rows.", []


def write_evidence_table_csv(path: Path, rows: List[Dict[str, Any]]) -> None:
    cols = [
        "claim_id","claim",
        "source_id","doi","title","year","venue","openalex_id",
        "relation","quote","quote_location",
        "certainty","certainty_reason",
    ]
    with path.open("w", newline="", encoding="utf-8") as f:
        w = csv.DictWriter(f, fieldnames=cols)
        w.writeheader()
        for r in rows:
            w.writerow({c: r.get(c, "") for c in cols})


def write_summary_md(path: Path, claims: List[str], table_rows: List[Dict[str, Any]], qc: Dict[str, Any]) -> None:
    by_claim: Dict[int, List[Dict[str, Any]]] = {}
    for r in table_rows:
        try:
            cid = int(str(r.get("claim_id", "")).strip())
        except Exception:
            continue
        by_claim.setdefault(cid, []).append(r)

    lines: List[str] = []
    lines.append("# Сводка доказательств (Stage C)\n")
    lines.append(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")

    lines.append("## QC корпуса (автоматическая проверка)\n")
    lines.append(f"- papers_total: {qc.get('papers_total')}\n")
    lines.append(f"- no_abstract_pct: {qc.get('no_abstract_pct')}\n")
    lines.append(f"- idea_domain: {qc.get('idea_domain')} (conf={qc.get('idea_conf')})\n")
    lines.append(f"- corpus_domain: {qc.get('corpus_domain')} (conf={qc.get('corpus_conf')})\n")
    if qc.get("topic_warning"):
        lines.append(f"- WARNING: {qc.get('topic_warning')}\n")
    lines.append("")

    for i, c in enumerate(claims, 1):
        rows = by_claim.get(i, [])
        sup = sum(1 for r in rows if str(r.get("relation","")).lower() == "supports")
        con = sum(1 for r in rows if str(r.get("relation","")).lower() == "contradicts")
        unc = sum(1 for r in rows if str(r.get("relation","")).lower() == "unclear")
        lines.append(f"## Утверждение {i}\n")
        lines.append(c + "\n")
        lines.append(f"- sources: {len(rows)} (supports={sup}, contradicts={con}, unclear={unc})\n")
        lines.append("")

    path.write_text("\n".join(lines), encoding="utf-8")


def main() -> int:
    ap = argparse.ArgumentParser()
    ap.add_argument("--idea", required=True)
    ap.add_argument("--claims-max", type=int, default=8)
    ap.add_argument("--k", type=int, default=6)
    ap.add_argument("--no-llm", action="store_true")
    ap.add_argument("--prompt-max-abs", type=int, default=1200)
    ap.add_argument("--no-topic-guard", action="store_true", help="Disable topic guard (not recommended).")
    args = ap.parse_args()

    idea_dir = Path(args.idea)
    in_dir, out_dir, logs_dir = idea_dir / "in", idea_dir / "out", idea_dir / "logs"
    ensure_dir(in_dir); ensure_dir(out_dir); ensure_dir(logs_dir)

    log_path = logs_dir / f"moduleC_{now_stamp()}.log"
    def log(msg: str) -> None:
        with log_path.open("a", encoding="utf-8") as f:
            f.write(msg + "\n")

    try:
        structured_path = out_dir / "structured_idea.json"
        corpus_path = out_dir / "corpus.csv"
        if not structured_path.exists():
            log("[ERROR] Missing input: out/structured_idea.json")
            return 1
        if not corpus_path.exists():
            log("[ERROR] Missing input: out/corpus.csv")
            return 1

        st = load_structured_idea(structured_path)
        claims = extract_claims(st, max_n=max(1, int(args.claims_max)))
        if not claims:
            log("[ERROR] Could not extract claims")
            return 1

        papers_all = load_corpus(corpus_path)
        if not papers_all:
            log("[ERROR] corpus.csv is empty")
            return 1

        idea_text = structured_idea_text(st)
        qc = estimate_domains(papers_all, idea_text)
        qc["papers_total"] = len(papers_all)
        target_domain = choose_target_domain(qc)

        if qc.get("idea_domain") != "unknown" and qc.get("corpus_domain") != "unknown":
            if qc.get("idea_domain") != qc.get("corpus_domain"):
                if float(qc.get("idea_conf", 0.0)) >= 0.34 and float(qc.get("corpus_conf", 0.0)) >= 0.34:
                    qc["topic_warning"] = "Корпус статей похож на другую область (возможно Stage B нашла 'мимо темы')."

        papers = papers_all
        if not args.no_topic_guard:
            papers = topic_guard_filter(papers_all, target_domain)

        log(f"[QC] papers_total={len(papers_all)} kept_after_guard={len(papers)} no_abstract_pct={qc.get('no_abstract_pct')}")
        log(f"[QC] idea_domain={qc.get('idea_domain')} (conf={qc.get('idea_conf')}) corpus_domain={qc.get('corpus_domain')} (conf={qc.get('corpus_conf')}) target_domain={target_domain}")
        if qc.get("topic_warning"):
            log("[QC] WARNING: " + str(qc.get("topic_warning")))

        idf = build_idf(papers)

        gt = tokenize(idea_text)
        global_hint = []
        seen = set()
        for t in gt:
            if t in seen: 
                continue
            seen.add(t)
            global_hint.append(t)
        global_hint = global_hint[:30]

        candidates: List[Dict[str, Any]] = []
        total_sources = 0

        for i, claim in enumerate(claims, 1):
            ranked = rank_papers_for_claim(claim, papers, idf, target_domain, global_hint_tokens=global_hint)
            top: List[Dict[str, Any]] = []
            used = set()
            for score, p in ranked:
                if len(top) >= int(args.k):
                    break
                if p.key in used:
                    continue
                used.add(p.key)

                abs_clean = re.sub(r"\s+", " ", (p.abstract or "")).strip()
                abs_trunc = abs_clean
                if len(abs_trunc) > int(args.prompt_max_abs):
                    abs_trunc = abs_trunc[: int(args.prompt_max_abs)] + "…"

                if abs_trunc:
                    text_for_llm = abs_trunc
                    quote_hint = "abstract"
                else:
                    text_for_llm = f"TITLE_ONLY: {p.title}"
                    quote_hint = "title_only"

                dom, conf = paper_domain(p)

                top.append({
                    "source_id": f"S{len(top)+1}",
                    "score": round(float(score), 4),
                    "openalex_id": p.openalex_id,
                    "doi": p.doi,
                    "title": p.title,
                    "year": p.year,
                    "venue": p.venue,
                    "cited_by": p.cited_by,
                    "abstract": abs_trunc,
                    "text_for_llm": text_for_llm,
                    "quote_hint": quote_hint,
                    "paper_domain": dom,
                    "paper_domain_conf": round(float(conf), 3),
                    "auto_snippet": snippet_around(abs_clean, tokenize(claim), max_len=260) if abs_clean else "",
                })

            total_sources += len(top)
            candidates.append({"claim_id": i, "claim": claim, "sources": top})

        (out_dir / "evidence_candidates.json").write_text(
            json.dumps({"meta": {"generated": now_stamp(), "qc": qc}, "candidates": candidates}, ensure_ascii=False, indent=2),
            encoding="utf-8",
        )

        if total_sources == 0:
            log("[ERROR] Не удалось подобрать источники: корпус вероятно 'мимо темы' (Stage B).")
            return 1

        if args.no_llm:
            rows: List[Dict[str, Any]] = []
            for c in candidates:
                for s in c.get("sources") or []:
                    quote = s.get("auto_snippet") or (s.get("abstract") or s.get("title") or "")
                    rows.append({
                        "claim_id": c.get("claim_id"),
                        "claim": c.get("claim"),
                        "source_id": s.get("source_id"),
                        "doi": s.get("doi"),
                        "title": s.get("title"),
                        "year": s.get("year"),
                        "venue": s.get("venue"),
                        "openalex_id": s.get("openalex_id"),
                        "relation": "unclear",
                        "quote": quote[:260],
                        "quote_location": "abstract/title (auto)",
                        "certainty": "Low",
                        "certainty_reason": "Auto (no LLM): candidate matching only",
                    })
            write_evidence_table_csv(out_dir / "evidence_table.csv", rows)
            write_summary_md(out_dir / "evidence_summary.md", claims, rows, qc)
            log("[OK] Module C complete (no-llm mode).")
            return 0

        llm_path = in_dir / "llm_evidence.json"
        prompt_path = out_dir / "llm_prompt_C.txt"

        if not llm_path.exists():
            llm_path.write_text(
                '{\n  "paste": "Paste ChatGPT JSON here (replace this file)",\n'
                '  "note": "Run RUN_C.bat again after saving valid JSON"\n}\n',
                encoding="utf-8",
            )

        raw = read_text_utf8_sig(llm_path) if llm_path.exists() else ""
        try:
            obj = extract_json(raw) if raw.strip() else {"paste": "Paste ChatGPT JSON here"}
        except Exception:
            obj = {"paste": "Paste ChatGPT JSON here (replace this file)", "note": "Run RUN_C.bat again after saving valid JSON"}

        ok, why, flat_rows = validate_llm(obj if isinstance(obj, dict) else {})
        if not ok:
            tpl = (Path(__file__).resolve().parents[1] / "config" / "prompts" / "llm_moduleC_prompt.txt")
            if not tpl.exists():
                log("[ERROR] Prompt template missing: " + str(tpl))
                return 1
            packet = json.dumps({"claims": candidates, "qc": qc}, ensure_ascii=False, indent=2)
            prompt = read_text_utf8_sig(tpl).replace("{{CANDIDATES_JSON}}", packet)
            prompt_path.write_text(prompt, encoding="utf-8")
            llm_path.write_text(json.dumps({
                "paste": "Paste ChatGPT JSON here (replace this file)",
                "note": "Run RUN_C.bat again after saving valid JSON",
                "reason_pipeline_waiting": why,
            }, ensure_ascii=False, indent=2), encoding="utf-8")
            log("[NEED] LLM evidence JSON missing/invalid: " + why)
            log("[NEED] Prompt generated: out/llm_prompt_C.txt")
            return 2

        # merge
        cand_by_claim: Dict[int, Dict[str, Dict[str, Any]]] = {}
        claim_text: Dict[int, str] = {}
        for c in candidates:
            cid = int(c.get("claim_id"))
            claim_text[cid] = str(c.get("claim"))
            m: Dict[str, Dict[str, Any]] = {}
            for s in (c.get("sources") or []):
                m[str(s.get("source_id"))] = s
            cand_by_claim[cid] = m

        out_rows: List[Dict[str, Any]] = []
        seen_pairs = set()
        for r in flat_rows:
            try:
                cid = int(str(r.get("claim_id", "")).strip())
            except Exception:
                continue
            sid = str(r.get("source_id") or "").strip()
            if not sid:
                continue
            if (cid, sid) in seen_pairs:
                continue
            seen_pairs.add((cid, sid))

            meta = (cand_by_claim.get(cid) or {}).get(sid)
            if not meta:
                continue

            rel = str(r.get("relation") or "unclear").strip().lower()
            if rel not in ("supports", "contradicts", "unclear"):
                rel = "unclear"
            cert = str(r.get("certainty") or "Low").strip()
            if cert not in ("High", "Med", "Low"):
                m = cert.lower()
                cert = "High" if m.startswith("h") else ("Med" if m.startswith("m") else "Low")

            quote = (r.get("quote") or meta.get("auto_snippet") or meta.get("abstract") or meta.get("title") or "")[:260]

            out_rows.append({
                "claim_id": cid,
                "claim": claim_text.get(cid, ""),
                "source_id": sid,
                "doi": meta.get("doi", ""),
                "title": meta.get("title", ""),
                "year": meta.get("year", ""),
                "venue": meta.get("venue", ""),
                "openalex_id": meta.get("openalex_id", ""),
                "relation": rel,
                "quote": quote,
                "quote_location": (r.get("quote_location") or meta.get("quote_hint") or "abstract").strip(),
                "certainty": cert,
                "certainty_reason": (r.get("certainty_reason") or "").strip(),
            })

        if not out_rows:
            log("[ERROR] LLM JSON parsed, but no rows matched candidate sources.")
            return 1

        write_evidence_table_csv(out_dir / "evidence_table.csv", out_rows)
        write_summary_md(out_dir / "evidence_summary.md", claims, out_rows, qc)
        log("[OK] Module C complete.")
        return 0

    except Exception as e:
        log("[ERROR] " + repr(e))
        log(traceback.format_exc())
        return 1


if __name__ == "__main__":
    raise SystemExit(main())