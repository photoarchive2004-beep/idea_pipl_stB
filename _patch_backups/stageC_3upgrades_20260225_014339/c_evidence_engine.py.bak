#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from __future__ import annotations

import argparse, csv, json, math, re, traceback
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple


def now_stamp() -> str:
    return datetime.now().strftime("%Y%m%d_%H%M%S")


def ensure_dir(p: Path) -> None:
    p.mkdir(parents=True, exist_ok=True)


def read_text_utf8_sig(path: Path) -> str:
    return path.read_text(encoding="utf-8-sig", errors="ignore")


def extract_json(text: str) -> dict:
    s = text.strip()
    if s.startswith("{") and s.endswith("}"):
        return json.loads(s)
    m = re.search(r"{[\s\S]*}", s)
    if not m:
        raise ValueError("No JSON object found")
    return json.loads(m.group(0))


def tokenize(text: str) -> List[str]:
    if not text:
        return []
    t = text.lower()
    toks = re.findall(r"[a-zа-яё][a-zа-яё0-9\-]{1,}", t, flags=re.IGNORECASE)
    stop = {
        "the","and","for","with","from","into","about","this","that","those","these",
        "have","has","had","were","was","are","is","be","been","being","study","studies",
        "result","results","method","methods","data","analysis","analyses","using","use","used",
        "based","across","between","within","among","via",
        "это","как","что","для","при","без","или","и","а","но","на","в","во","по","из","к","у","о","об","про","над","под","между","с","со","же","ли","не","да","нет",
    }
    out: List[str] = []
    for x in toks:
        x = x.strip("-")
        if len(x) < 3:
            continue
        if x in stop:
            continue
        out.append(x)
    return out


DOMAIN_KWS: Dict[str, List[str]] = {
    # broad domains, to stay universal
    "biomed": ["patient","clinical","therapy","treatment","tumor","cancer","microbiome","antibiotic","infection","bacteria",
               "пациент","клинич","терап","лечен","рак","опухол","антибиот","инфекц","микробиот"],
    "eco_evo": ["species","population","phylog","evolution","ecology","biodiversity","habitat","river","watershed","landscape","riverscape",
                "genetic","genome","wgs","ddrad","radseq","mtdna","introgression","speciation","glacial","fish","minnow",
                "вид","популяц","эволюц","эколог","ареал","река","бассейн","ландшафт","генетик","геном","мтднк","рыб","гольян"],
    "social": ["survey","policy","education","econom","market","governance","psycholog","behavior",
               "опрос","политик","образован","эконом","рынок","социолог","психолог","поведен"],
    "tech": ["algorithm","machine","learning","neural","network","computer","software","hardware","sensor","optimization","numerical","simulation",
             "алгоритм","машинн","обучен","нейрон","компьют","софт","датчик","оптимиз","числен","моделирован"],
    "other": []
}


def domain_scores(text: str) -> Dict[str, float]:
    toks = set(tokenize(text))
    scores: Dict[str, float] = {k: 0.0 for k in DOMAIN_KWS.keys()}
    if not toks:
        return scores
    for dom, kws in DOMAIN_KWS.items():
        s = 0.0
        for kw in kws:
            if kw in toks:
                s += 1.0
            elif any(kw in t for t in toks):
                s += 0.6
        scores[dom] = s
    return scores


def pick_top_domains(scores: Dict[str, float]) -> Tuple[List[str], float]:
    items = [(k, float(v)) for k, v in scores.items()]
    items.sort(key=lambda x: x[1], reverse=True)
    best_dom, best = items[0]
    total = sum(v for _, v in items) or 1.0
    conf = best / total

    # allow 2 domains if close -> supports interdisciplinary ideas
    allowed = [best_dom]
    if len(items) > 1:
        second_dom, second = items[1]
        if second > 0 and (second / (best + 1e-9)) >= 0.75:
            allowed.append(second_dom)

    # if everything is ~0, return "other"
    if best < 1.0:
        return ["other"], 0.0
    return allowed, conf


@dataclass
class Paper:
    openalex_id: str
    doi: str
    title: str
    year: str
    venue: str
    cited_by: int
    abstract: str

    @property
    def key(self) -> str:
        return self.doi or self.openalex_id or self.title


def load_corpus(path: Path) -> List[Paper]:
    rows: List[Paper] = []
    with path.open("r", encoding="utf-8-sig", errors="replace", newline="") as f:
        r = csv.DictReader(f)
        for row in r:
            doi = (row.get("doi") or "").strip()
            if doi.startswith("https://doi.org/"):
                doi = doi.replace("https://doi.org/", "")
            try:
                cb = int(float((row.get("cited_by") or "0").strip() or 0))
            except Exception:
                cb = 0
            rows.append(Paper(
                openalex_id=(row.get("openalex_id") or "").strip(),
                doi=doi,
                title=(row.get("title") or "").strip(),
                year=(row.get("year") or "").strip(),
                venue=(row.get("venue") or "").strip(),
                cited_by=cb,
                abstract=(row.get("abstract") or "").strip(),
            ))
    return rows


def build_idf(papers: List[Paper]) -> Dict[str, float]:
    df: Dict[str, int] = {}
    n = len(papers)
    for p in papers:
        toks = set(tokenize((p.title or "") + " " + (p.abstract or "")))
        for t in toks:
            df[t] = df.get(t, 0) + 1
    idf: Dict[str, float] = {}
    for t, d in df.items():
        idf[t] = math.log((n + 1) / (d + 1)) + 1.0
    return idf


def load_structured_idea(path: Path) -> Dict[str, Any]:
    obj = json.loads(read_text_utf8_sig(path))
    if isinstance(obj, dict) and "structured_idea" in obj:
        return obj
    return {"structured_idea": obj}


def structured_idea_text(obj: Dict[str, Any]) -> str:
    si = obj.get("structured_idea") or {}
    parts: List[str] = []
    for k in ["title","problem","background","motivation","main_hypothesis","alternative_hypotheses","key_predictions","claims_to_verify","methods_minimal","data_sources","keywords"]:
        v = si.get(k)
        if isinstance(v, list):
            parts.extend([str(x) for x in v if str(x).strip()])
        elif v:
            parts.append(str(v))
    return "\n".join(parts)


def extract_claims(obj: Dict[str, Any], max_n: int) -> List[str]:
    si = obj.get("structured_idea") or {}
    claims: List[str] = []
    for k in ["claims_to_verify","key_predictions"]:
        v = si.get(k)
        if isinstance(v, list):
            claims.extend([str(x).strip() for x in v if str(x).strip()])
    if not claims:
        for k in ["main_hypothesis","problem"]:
            v = si.get(k)
            if v:
                claims.append(str(v).strip())
        alts = si.get("alternative_hypotheses")
        if isinstance(alts, list):
            claims.extend([str(x).strip() for x in alts if str(x).strip()])
    # uniq
    seen=set(); out=[]
    for c in claims:
        c=re.sub(r"\s+"," ",c).strip()
        if not c: continue
        cl=c.lower()
        if cl in seen: continue
        seen.add(cl); out.append(c)
    return out[:max_n]


def paper_domain(p: Paper) -> Tuple[str, float]:
    sc = domain_scores((p.title or "") + " " + (p.abstract or ""))
    allowed, conf = pick_top_domains(sc)
    # return top1 + its confidence proxy
    return allowed[0], float(conf)


def estimate_qc(papers: List[Paper], idea_text: str) -> Dict[str, Any]:
    idea_sc = domain_scores(idea_text)
    idea_allowed, idea_conf = pick_top_domains(idea_sc)

    counts = {k: 0 for k in DOMAIN_KWS.keys()}
    no_abs = 0
    for p in papers:
        if not (p.abstract or "").strip():
            no_abs += 1
        d, _ = paper_domain(p)
        counts[d] = counts.get(d, 0) + 1

    corpus_top = max(counts.items(), key=lambda x: x[1])[0] if papers else "other"
    corpus_conf = (counts.get(corpus_top, 0) / max(1, len(papers))) if papers else 0.0

    return {
        "papers_total": len(papers),
        "no_abstract_papers": no_abs,
        "no_abstract_pct": round(no_abs / max(1, len(papers)), 3),
        "idea_domains": idea_allowed,
        "idea_conf": round(float(idea_conf), 3),
        "corpus_domain": corpus_top,
        "corpus_conf": round(float(corpus_conf), 3),
        "domain_counts": counts,
    }


def allowed_domains_from_qc(qc: Dict[str, Any]) -> List[str]:
    idea_domains = qc.get("idea_domains") or ["other"]
    idea_conf = float(qc.get("idea_conf", 0.0))
    # if idea is confident -> trust it; else fall back to corpus majority
    if idea_domains and idea_domains != ["other"] and idea_conf >= 0.35:
        return list(dict.fromkeys(idea_domains))
    cd = qc.get("corpus_domain") or "other"
    return [cd]


def filter_by_domain(papers: List[Paper], allowed: List[str]) -> List[Paper]:
    if not allowed or allowed == ["other"]:
        return papers
    out=[]
    for p in papers:
        d, conf = paper_domain(p)
        # drop if clearly out-of-domain
        if d not in allowed and conf >= 0.45:
            continue
        out.append(p)
    return out


def anchors_from_idea(idea_text: str, idf: Dict[str, float], min_idf: float = 2.2, k: int = 25) -> List[str]:
    toks = tokenize(idea_text)
    uniq=[]
    seen=set()
    for t in toks:
        if t in seen: 
            continue
        seen.add(t)
        uniq.append(t)
    scored=[(idf.get(t,0.0),t) for t in uniq]
    scored.sort(key=lambda x:x[0], reverse=True)
    anchors=[t for s,t in scored if s >= min_idf][:k]
    return anchors


def anchor_hit_count(p: Paper, anchors: List[str]) -> int:
    if not anchors:
        return 0
    txt = " ".join(tokenize((p.title or "") + " " + (p.abstract or "")))
    return sum(1 for a in anchors if a in txt)


def rank_for_claim(claim: str, papers: List[Paper], idf: Dict[str, float], anchors: List[str]) -> List[Tuple[float, Paper, int]]:
    ct=set(tokenize(claim))
    ranked=[]
    for p in papers:
        toks=set(tokenize((p.title or "") + " " + (p.abstract or "")))
        inter = ct & toks
        if not inter and anchors:
            # if claim is generic, still allow papers that match anchors
            pass
        base = sum(idf.get(t, 1.0) for t in inter)
        ah = anchor_hit_count(p, anchors)
        # anchor boost prevents "adaptation" pulling marketing etc.
        boost = ah * 0.9
        score = base + boost
        if p.cited_by > 0:
            score *= (1.0 + min(0.35, math.log10(p.cited_by + 1) / 6.0))
        if score > 0:
            ranked.append((score, p, ah))
    ranked.sort(key=lambda x:x[0], reverse=True)
    return ranked


def validate_llm(obj: Dict[str, Any]) -> Tuple[bool, str, List[Dict[str, Any]]]:
    if not isinstance(obj, dict):
        return False, "Response is not a JSON object.", []
    rows = obj.get("evidence_rows")
    if isinstance(rows, list) and any(isinstance(x, dict) for x in rows):
        flat=[x for x in rows if isinstance(x, dict)]
        return (len(flat)>0), ("OK" if flat else "evidence_rows empty"), flat
    return False, "Missing evidence_rows.", []


def write_csv(path: Path, rows: List[Dict[str, Any]]) -> None:
    cols=["claim_id","claim","source_id","doi","title","year","venue","openalex_id","relation","quote","quote_location","certainty","certainty_reason"]
    with path.open("w", newline="", encoding="utf-8") as f:
        w=csv.DictWriter(f, fieldnames=cols)
        w.writeheader()
        for r in rows:
            w.writerow({c:r.get(c,"") for c in cols})


def write_md(path: Path, claims: List[str], rows: List[Dict[str, Any]], qc: Dict[str, Any]) -> None:
    by={}
    for r in rows:
        try: cid=int(str(r.get("claim_id","")).strip())
        except: continue
        by.setdefault(cid, []).append(r)
    lines=[]
    lines.append("# Сводка доказательств (Stage C)\n")
    lines.append("## QC корпуса\n")
    for k in ["papers_total","no_abstract_pct","idea_domains","idea_conf","corpus_domain","corpus_conf","domain_counts","allowed_domains","anchors"]:
        if k in qc:
            lines.append(f"- {k}: {qc[k]}")
    lines.append("")
    for i,c in enumerate(claims,1):
        rr=by.get(i,[])
        sup=sum(1 for x in rr if str(x.get("relation","")).lower()=="supports")
        con=sum(1 for x in rr if str(x.get("relation","")).lower()=="contradicts")
        unc=sum(1 for x in rr if str(x.get("relation","")).lower()=="unclear")
        lines.append(f"## Утверждение {i}\n{c}\n")
        lines.append(f"- sources: {len(rr)} (supports={sup}, contradicts={con}, unclear={unc})\n")
    path.write_text("\n".join(lines), encoding="utf-8")


def main() -> int:
    ap=argparse.ArgumentParser()
    ap.add_argument("--idea", required=True)
    ap.add_argument("--claims-max", type=int, default=8)
    ap.add_argument("--k", type=int, default=6)
    ap.add_argument("--no-llm", action="store_true")
    ap.add_argument("--prompt-max-abs", type=int, default=1200)
    args=ap.parse_args()

    idea_dir=Path(args.idea)
    in_dir, out_dir, logs_dir = idea_dir/"in", idea_dir/"out", idea_dir/"logs"
    ensure_dir(in_dir); ensure_dir(out_dir); ensure_dir(logs_dir)
    log_path=logs_dir/f"moduleC_{now_stamp()}.log"

    def log(s:str)->None:
        with log_path.open("a", encoding="utf-8") as f:
            f.write(s+"\n")

    try:
        structured=out_dir/"structured_idea.json"
        corpus=out_dir/"corpus.csv"
        if not structured.exists():
            log("[ERROR] Missing out/structured_idea.json"); return 1
        if not corpus.exists():
            log("[ERROR] Missing out/corpus.csv"); return 1

        st=load_structured_idea(structured)
        idea_text=structured_idea_text(st)
        claims=extract_claims(st, max_n=max(1,int(args.claims_max)))
        if not claims:
            log("[ERROR] Could not extract claims"); return 1

        papers_all=load_corpus(corpus)
        if not papers_all:
            log("[ERROR] corpus.csv empty"); return 1

        qc=estimate_qc(papers_all, idea_text)
        allowed=allowed_domains_from_qc(qc)
        qc["allowed_domains"]=allowed

        # domain filter (eligibility)
        papers = filter_by_domain(papers_all, allowed)

        idf=build_idf(papers if papers else papers_all)
        anchors=anchors_from_idea(idea_text, idf)
        qc["anchors"]=anchors

        if not papers:
            papers = papers_all  # fallback
            log("[QC] WARNING: after domain filter no papers left, fallback to full corpus.")

        log(f"[QC] papers_total={qc['papers_total']} kept_after_domain={len(papers)} allowed_domains={allowed} no_abstract_pct={qc['no_abstract_pct']}")
        log(f"[QC] domain_counts={qc['domain_counts']} idea_domains={qc['idea_domains']} idea_conf={qc['idea_conf']}")

        candidates=[]
        total=0
        for i, claim in enumerate(claims,1):
            ranked = rank_for_claim(claim, papers, idf, anchors)
            top=[]
            used=set()
            for score, p, ah in ranked:
                if len(top) >= int(args.k):
                    break
                if p.key in used:
                    continue
                used.add(p.key)
                abs_clean=re.sub(r"\s+"," ",(p.abstract or "")).strip()
                if abs_clean and len(abs_clean) > int(args.prompt_max_abs):
                    abs_clean = abs_clean[:int(args.prompt_max_abs)]+"…"
                if abs_clean:
                    text_for_llm=abs_clean
                    quote_hint="abstract"
                else:
                    text_for_llm=f"TITLE_ONLY: {p.title}"
                    quote_hint="title"
                dom, conf = paper_domain(p)
                top.append({
                    "source_id": f"S{len(top)+1}",
                    "score": round(float(score),4),
                    "anchor_hits": int(ah),
                    "paper_domain": dom,
                    "paper_domain_conf": round(float(conf),3),
                    "openalex_id": p.openalex_id,
                    "doi": p.doi,
                    "title": p.title,
                    "year": p.year,
                    "venue": p.venue,
                    "cited_by": p.cited_by,
                    "abstract": abs_clean,
                    "text_for_llm": text_for_llm,
                    "quote_hint": quote_hint,
                })
            total += len(top)
            candidates.append({"claim_id": i, "claim": claim, "sources": top})

        out_dir.joinpath("evidence_candidates.json").write_text(
            json.dumps({"claims": candidates, "qc": qc}, ensure_ascii=False, indent=2),
            encoding="utf-8"
        )

        if total == 0:
            log("[ERROR] No candidates found (likely corpus too off-topic)."); return 1

        # LLM handoff
        llm_json = in_dir/"llm_evidence.json"
        prompt_txt = out_dir/"llm_prompt_C.txt"

        if args.no_llm:
            # minimal fallback: everything unclear
            rows=[]
            for c in candidates:
                for s in c["sources"]:
                    rows.append({
                        "claim_id": c["claim_id"], "claim": c["claim"],
                        "source_id": s["source_id"], "doi": s.get("doi",""), "title": s.get("title",""),
                        "year": s.get("year",""), "venue": s.get("venue",""), "openalex_id": s.get("openalex_id",""),
                        "relation": "unclear", "quote": (s.get("abstract") or s.get("title") or "")[:260],
                        "quote_location": "abstract|title", "certainty": "Low", "certainty_reason": "no_llm_mode"
                    })
            write_csv(out_dir/"evidence_table.csv", rows)
            write_md(out_dir/"evidence_summary.md", claims, rows, qc)
            log("[OK] no-llm complete"); return 0

        if not llm_json.exists():
            llm_json.write_text('{"paste":"Paste ChatGPT JSON here (replace this file)"}\n', encoding="utf-8")

        raw = read_text_utf8_sig(llm_json)
        try:
            obj = extract_json(raw) if raw.strip() else {"paste":"Paste ChatGPT JSON here"}
        except Exception:
            obj = {"paste":"Paste ChatGPT JSON here"}

        ok, why, flat = validate_llm(obj)
        if not ok:
            tpl = Path(__file__).resolve().parents[1]/"config"/"prompts"/"llm_moduleC_prompt.txt"
            if not tpl.exists():
                log("[ERROR] missing prompt template"); return 1
            packet = json.dumps({"claims": candidates, "qc": qc}, ensure_ascii=False, indent=2)
            prompt = read_text_utf8_sig(tpl).replace("{{CANDIDATES_JSON}}", packet)
            prompt_txt.write_text(prompt, encoding="utf-8")
            llm_json.write_text(json.dumps({"paste":"Paste ChatGPT JSON here (replace this file)","reason_pipeline_waiting":why}, ensure_ascii=False, indent=2), encoding="utf-8")
            log("[NEED] LLM: " + why)
            return 2

        # merge back
        cand_map={}
        claim_text={}
        for c in candidates:
            cid=int(c["claim_id"])
            claim_text[cid]=c["claim"]
            cand_map[cid]={s["source_id"]: s for s in c["sources"]}

        out_rows=[]
        seen=set()
        for r in flat:
            try: cid=int(str(r.get("claim_id","")).strip())
            except: continue
            sid=str(r.get("source_id","")).strip()
            if not sid or (cid,sid) in seen: 
                continue
            seen.add((cid,sid))
            meta = cand_map.get(cid,{}).get(sid)
            if not meta: 
                continue
            rel=str(r.get("relation","unclear")).lower().strip()
            if rel not in ("supports","contradicts","unclear"):
                rel="unclear"
            cert=str(r.get("certainty","Low")).strip()
            if cert not in ("High","Med","Low"):
                cert="Low"
            quote=(r.get("quote") or meta.get("abstract") or meta.get("title") or "")[:260]
            out_rows.append({
                "claim_id": cid, "claim": claim_text.get(cid,""),
                "source_id": sid, "doi": meta.get("doi",""), "title": meta.get("title",""),
                "year": meta.get("year",""), "venue": meta.get("venue",""), "openalex_id": meta.get("openalex_id",""),
                "relation": rel, "quote": quote, "quote_location": (r.get("quote_location") or meta.get("quote_hint") or "abstract"),
                "certainty": cert, "certainty_reason": (r.get("certainty_reason") or "").strip()
            })

        if not out_rows:
            log("[ERROR] LLM JSON parsed but no rows matched."); return 1

        write_csv(out_dir/"evidence_table.csv", out_rows)
        write_md(out_dir/"evidence_summary.md", claims, out_rows, qc)
        log("[OK] Module C complete"); return 0

    except Exception as e:
        log("[ERROR] " + repr(e))
        log(traceback.format_exc())
        return 1


if __name__ == "__main__":
    raise SystemExit(main())